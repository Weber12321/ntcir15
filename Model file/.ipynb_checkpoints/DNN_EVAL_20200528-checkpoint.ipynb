{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DialEval-1 DNN practice (with evaluation function)\n",
    "### Model : CNN, LSTM\n",
    "##### input round, sender pred label\n",
    "##### Edited by Weber Huang in 2020-05-28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Eval function and customizes loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(pred, truth):\n",
    "    \"\"\" convert inputs to np.array and make sure\n",
    "    inputs are normalized probability distributions\n",
    "    \"\"\"\n",
    "    if len(pred) != len(truth):\n",
    "        raise ValueError(\"pred and truth have different lengths\")\n",
    "    if len(pred) == 0 or len(truth) == 0:\n",
    "        raise ValueError(\"pred or truth are empty\")\n",
    "\n",
    "    pred, truth = np.asarray(pred), np.asarray(truth)\n",
    "    if not ((pred >= 0).all() and (truth >= 0).all()):\n",
    "        raise ValueError(\"probability distribution should not be negative\")\n",
    "    pred, truth = pred / pred.sum(), truth / truth.sum()\n",
    "    return pred, truth\n",
    "\n",
    "def jensen_shannon_div(pred, truth, base=2):\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    m = 1. / 2 * (pred + truth)\n",
    "    return (stats.entropy(pred, m, base=base)\n",
    "            + stats.entropy(truth, m, base=base)) / 2.\n",
    "\n",
    "def root_normalized_squared_error(pred, truth):\n",
    "    \"\"\" RNSS: Root Normalised Sum of Squares\n",
    "    \"\"\"\n",
    "\n",
    "    def squared_error(pred, truth):\n",
    "        return ((pred - truth) ** 2).sum()\n",
    "\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    return np.sqrt(squared_error(pred, truth) / 2)\n",
    "\n",
    "def jsd_custom_loss(y_true, y_pred):\n",
    "            \n",
    "    # calculate loss, using y_pred\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "#     y_pred, y_true = normalize(y_pred, y_true)\n",
    "    m = 1. / 2 * (y_pred + y_true)\n",
    "    # loss = (stats.entropy(y_pred, m, base=2) + stats.entropy(y_true, m, base=2)) / 2.\n",
    "    # tf.keras.losses.KLD()\n",
    "    loss = (tf.keras.losses.KLD(y_pred, m) + tf.keras.losses.KLD(y_true, m)) / 2.\n",
    "    return loss\n",
    "  \n",
    "# model.compile(loss=jsd_custom_loss, optimizer='adam')\n",
    "\n",
    "def rnss_custom_loss(y_true, y_pred):\n",
    "            \n",
    "    # calculate loss, using y_pred\n",
    "    \"\"\" RNSS: Root Normalised Sum of Squares\n",
    "    \"\"\"\n",
    "\n",
    "    def squared_error(y_pred, y_true):\n",
    "        return ((y_pred - y_true) ** 2).sum()\n",
    "\n",
    "#     y_pred, y_true = normalize(y_pred, y_true)\n",
    "    loss = np.sqrt(squared_error(y_pred, y_true) / 2)\n",
    "    \n",
    "    return loss\n",
    "  \n",
    "# model.compile(loss=custom_loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. input dataset and modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/DialEval-1')\n",
    "df = pd.read_excel(r'./200514_dev+train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>round</th>\n",
       "      <th>sender</th>\n",
       "      <th>texts</th>\n",
       "      <th>max_label</th>\n",
       "      <th>round_label</th>\n",
       "      <th>CNUG</th>\n",
       "      <th>CNUG*</th>\n",
       "      <th>CNUG0</th>\n",
       "      <th>CNaN</th>\n",
       "      <th>HNUG</th>\n",
       "      <th>HNUG*</th>\n",
       "      <th>HNaN</th>\n",
       "      <th>sender_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4227729258237820</td>\n",
       "      <td>1</td>\n",
       "      <td>customer</td>\n",
       "      <td>内涵 段子 联通 皮 点赞 中国联通 中国联通 客服 掌上 营业厅 内涵 段子 话题 封 郑...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4227729258237820</td>\n",
       "      <td>2</td>\n",
       "      <td>helpdesk</td>\n",
       "      <td>u</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4227729258237820</td>\n",
       "      <td>3</td>\n",
       "      <td>customer</td>\n",
       "      <td>夸夸</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4227729258237820</td>\n",
       "      <td>4</td>\n",
       "      <td>helpdesk</td>\n",
       "      <td>*</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4121001149457180</td>\n",
       "      <td>1</td>\n",
       "      <td>customer</td>\n",
       "      <td>距离 反映 问题 已经 一个 星期 花粉 助手 D 荣耀 honor 荣耀 手机 华为 终端...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  round    sender  \\\n",
       "0  4227729258237820      1  customer   \n",
       "1  4227729258237820      2  helpdesk   \n",
       "2  4227729258237820      3  customer   \n",
       "3  4227729258237820      4  helpdesk   \n",
       "4  4121001149457180      1  customer   \n",
       "\n",
       "                                               texts  max_label  round_label  \\\n",
       "0  内涵 段子 联通 皮 点赞 中国联通 中国联通 客服 掌上 营业厅 内涵 段子 话题 封 郑...          3            2   \n",
       "1                                                  u          6            4   \n",
       "2                                                 夸夸          3            0   \n",
       "3                                                  *          6            4   \n",
       "4  距离 反映 问题 已经 一个 星期 花粉 助手 D 荣耀 honor 荣耀 手机 华为 终端...          2            2   \n",
       "\n",
       "       CNUG  CNUG*     CNUG0      CNaN      HNUG  HNUG*      HNaN  sender_num  \n",
       "0  0.052632    0.0  0.157895  0.789474  0.000000    0.0  0.000000           0  \n",
       "1  0.000000    0.0  0.000000  0.000000  0.157895    0.0  0.842105           1  \n",
       "2  0.157895    0.0  0.000000  0.842105  0.000000    0.0  0.000000           0  \n",
       "3  0.000000    0.0  0.000000  0.000000  0.157895    0.0  0.842105           1  \n",
       "4  0.052632    0.0  0.789474  0.157895  0.000000    0.0  0.000000           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = []\n",
    "for i in df['sender']:\n",
    "    if i == 'customer':\n",
    "        tmp.append(0)\n",
    "    else:\n",
    "        tmp.append(1)\n",
    "df['sender_num'] = tmp\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17155, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "dev = df[0:1754]\n",
    "train = df[1755:]\n",
    "\n",
    "X_train = train.filter(['round','sender_num','round_label','texts'])\n",
    "X_test = dev.filter(['round','sender_num','round_label','texts'])\n",
    "y_train = train.filter(['CNUG','CNUG*','CNUG0','CNaN','HNUG','HNUG*','HNaN'])\n",
    "y_test = dev.filter(['CNUG','CNUG*','CNUG0','CNaN','HNUG','HNUG*','HNaN'])\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN(X_train, X_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "    \n",
    "    X1_train = X_train['texts']\n",
    "#     X1_train = X_train[:,[-1]]\n",
    "    X1_train = [str (item) for item in X1_train]\n",
    "    \n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = X_test[:,[-1]]\n",
    "    X1_test = [str (item) for item in X1_test]\n",
    "\n",
    "    X2_train = X_train[['round','sender_num','round_label']].values\n",
    "#     X2_train = X_train[:,0:7]\n",
    "    X2_test = X_test[['round','sender_num','round_label']].values\n",
    "#     X2_test = X_test[:,0:7]\n",
    "    \n",
    "    token = Tokenizer(num_words = 20000)\n",
    "    token.fit_on_texts(X1_train)\n",
    "    vocab = token.word_index\n",
    "    print(token.document_count)\n",
    "\n",
    "    x_train_seq = token.texts_to_sequences(X1_train)\n",
    "    x_test_seq = token.texts_to_sequences(X1_test)\n",
    "    X1_train = sequence.pad_sequences(x_train_seq, maxlen = 150)\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 150)\n",
    "\n",
    "#     y_one_train = np_utils.to_categorical(y_train)\n",
    "#     y_one_test = np_utils.to_categorical(y_test)\n",
    "     \n",
    "    num_labels = 7\n",
    "    main_input = Input(shape=(150,), dtype='float64')\n",
    "\n",
    "    sub_input = Input(shape=(3,))\n",
    "    \n",
    "    # pre-train embeddings\n",
    "    # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "    # embed = embedder(main_input)\n",
    "    embed = Embedding(len(vocab)+1, 300, input_length=150)(main_input)\n",
    "    # filter size, region size\n",
    "    cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "    cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    # main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "\n",
    "\n",
    "    dense_1 = Dense(units=256,activation='relu')(sub_input)\n",
    "    drop_1 = Dropout(0.35)(dense_1)\n",
    "    dense_2 = Dense(units=128,activation='relu')(drop_1)\n",
    "    # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "    merge = concatenate([drop, dense_2])\n",
    "    dense_3 = Dense(units=10, activation='relu')(merge)\n",
    "    output = Dense(units=7, activation='softmax')(dense_3)\n",
    "\n",
    "    model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "                              batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "    score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "    print(\"Test Score:\", score[0])\n",
    "    print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "    pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "    return train_history, pre_probability\n",
    "    '''\n",
    "    from sklearn import metrics\n",
    "    print(\"Classification report for classifier:\\n%s\\n\"\n",
    "          % ( metrics.classification_report(y_test, predicted)))\n",
    "\n",
    "    print(\"f1_score :\\n%s\\n\" % ( metrics.f1_score(y_test, predicted, average='macro')))\n",
    "    print(\"acc_score :\\n%s\\n\" % ( metrics.accuracy_score(y_test, predicted)))\n",
    "    '''\n",
    "\n",
    "def lstm(X_train, X_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "    X1_train = X_train['texts']\n",
    "#     X1_train = X_train[:,[-1]]\n",
    "    X1_train = [str (item) for item in X1_train]\n",
    "    \n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = X_test[:,[-1]]\n",
    "    X1_test = [str (item) for item in X1_test]\n",
    "\n",
    "    X2_train = X_train[['round','sender_num','round_label']].values\n",
    "#     X2_train = X_train[:,0:7]\n",
    "    X2_test = X_test[['round','sender_num','round_label']].values\n",
    "#     X2_test = X_test[:,0:7]\n",
    "    \n",
    "    token = Tokenizer(num_words = 20000)\n",
    "    token.fit_on_texts(X1_train)\n",
    "    print(token.document_count)\n",
    "\n",
    "    x_train_seq = token.texts_to_sequences(X1_train)\n",
    "    x_test_seq = token.texts_to_sequences(X1_test)\n",
    "    X1_train = sequence.pad_sequences(x_train_seq, maxlen = 150)\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 150)\n",
    "\n",
    "#     y_one_train = np_utils.to_categorical(y_train)\n",
    "#     y_one_test = np_utils.to_categorical(y_test)\n",
    "     \n",
    "    main_input = Input(shape=(150,), dtype='float64')\n",
    "    sub_input = Input(shape=(3,))\n",
    "    \n",
    "    embed = Embedding(output_dim=32,input_dim=20000,input_length=150)(main_input)\n",
    "    dropout_1 = Dropout(0.35)(embed)\n",
    "    lst = LSTM(units=16)(dropout_1)\n",
    "    dense_1 = Dense(units=256,activation='relu')(lst)\n",
    "    dropout_2 = Dropout(0.35)(dense_1)\n",
    "    dense_2 = Dense(units=128,activation='relu')(dropout_2)\n",
    "    dense_3 = Dense(units=7,activation='softmax')(dense_2)\n",
    "\n",
    "\n",
    "    dense_4 = Dense(units=256,activation='relu')(sub_input)\n",
    "    dropout_3 = Dropout(0.35)(dense_4)\n",
    "    dense_5 = Dense(units=128,activation='relu')(dropout_3)\n",
    "    # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "    merge = concatenate([dense_3, dense_5])\n",
    "    dense_6 = Dense(units=10, activation='relu')(merge)\n",
    "    output = Dense(units=7, activation='softmax')(dense_6)\n",
    "\n",
    "    model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "                              batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "    score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "    print(\"Test Score:\", score[0])\n",
    "    print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "    pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "    return train_history, pre_probability\n",
    "    '''\n",
    "    from sklearn import metrics\n",
    "    print(\"Classification report for classifier:\\n%s\\n\"\n",
    "          % ( metrics.classification_report(y_test, predicted)))\n",
    "\n",
    "    print(\"f1_score :\\n%s\\n\" % ( metrics.f1_score(y_test, predicted, average='macro')))\n",
    "    print(\"acc_score :\\n%s\\n\" % ( metrics.accuracy_score(y_test, predicted)))\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_folds = 3\\nmodel_history = []\\nfor i in tqdm(range(n_folds)):\\n    print(\"Training on Fold: \",i+1)\\n    from sklearn.model_selection import train_test_split\\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, \\n                                                        random_state = np.random.randint(1,1000, 1)[0])\\n                                               \\n    \\n    model_history.append(CNN(X_train, X_test, y_train, y_test))\\n    print(\"=======\"*12, end=\"\\n\\n\\n\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "'''\n",
    "n_folds = 3\n",
    "model_history = []\n",
    "for i in tqdm(range(n_folds)):\n",
    "    print(\"Training on Fold: \",i+1)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, \n",
    "                                                        random_state = np.random.randint(1,1000, 1)[0])\n",
    "                                               \n",
    "    \n",
    "    model_history.append(CNN(X_train, X_test, y_train, y_test))\n",
    "    print(\"=======\"*12, end=\"\\n\\n\\n\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15400\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     6533100     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 150, 2)       1202        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 37, 2)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1024        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 74)           0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 74)           0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 202)          0           dropout_1[0][0]                  \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           2030        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7)            77          dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,570,329\n",
      "Trainable params: 6,570,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 12320 samples, validate on 3080 samples\n",
      "Epoch 1/50\n",
      " - 5s - loss: 1.0512 - accuracy: 0.7805 - val_loss: 0.8226 - val_accuracy: 0.8513\n",
      "Epoch 2/50\n",
      " - 3s - loss: 0.8244 - accuracy: 0.8350 - val_loss: 0.8031 - val_accuracy: 0.8526\n",
      "Epoch 3/50\n",
      " - 4s - loss: 0.8054 - accuracy: 0.8366 - val_loss: 0.7976 - val_accuracy: 0.8578\n",
      "Epoch 4/50\n",
      " - 3s - loss: 0.7937 - accuracy: 0.8375 - val_loss: 0.7969 - val_accuracy: 0.8565\n",
      "Epoch 5/50\n",
      " - 3s - loss: 0.7870 - accuracy: 0.8373 - val_loss: 0.7955 - val_accuracy: 0.8610\n",
      "Epoch 6/50\n",
      " - 3s - loss: 0.7807 - accuracy: 0.8420 - val_loss: 0.7980 - val_accuracy: 0.8523\n",
      "Epoch 7/50\n",
      " - 2s - loss: 0.7765 - accuracy: 0.8451 - val_loss: 0.7982 - val_accuracy: 0.8536\n",
      "Epoch 8/50\n",
      " - 3s - loss: 0.7717 - accuracy: 0.8480 - val_loss: 0.7951 - val_accuracy: 0.8542\n",
      "Epoch 9/50\n",
      " - 3s - loss: 0.7694 - accuracy: 0.8494 - val_loss: 0.7976 - val_accuracy: 0.8536\n",
      "Epoch 10/50\n",
      " - 2s - loss: 0.7667 - accuracy: 0.8515 - val_loss: 0.7978 - val_accuracy: 0.8494\n",
      "Epoch 11/50\n",
      " - 3s - loss: 0.7651 - accuracy: 0.8545 - val_loss: 0.7983 - val_accuracy: 0.8581\n",
      "Epoch 12/50\n",
      " - 3s - loss: 0.7632 - accuracy: 0.8535 - val_loss: 0.7985 - val_accuracy: 0.8552\n",
      "Epoch 13/50\n",
      " - 3s - loss: 0.7624 - accuracy: 0.8546 - val_loss: 0.7973 - val_accuracy: 0.8597\n",
      "Epoch 14/50\n",
      " - 3s - loss: 0.7626 - accuracy: 0.8528 - val_loss: 0.8005 - val_accuracy: 0.8523\n",
      "Epoch 15/50\n",
      " - 3s - loss: 0.7600 - accuracy: 0.8559 - val_loss: 0.8023 - val_accuracy: 0.8601\n",
      "Epoch 16/50\n",
      " - 3s - loss: 0.7591 - accuracy: 0.8565 - val_loss: 0.8035 - val_accuracy: 0.8597\n",
      "Epoch 17/50\n",
      " - 3s - loss: 0.7587 - accuracy: 0.8526 - val_loss: 0.7999 - val_accuracy: 0.8584\n",
      "Epoch 18/50\n",
      " - 3s - loss: 0.7562 - accuracy: 0.8609 - val_loss: 0.8041 - val_accuracy: 0.8588\n",
      "Epoch 19/50\n",
      " - 3s - loss: 0.7557 - accuracy: 0.8569 - val_loss: 0.8015 - val_accuracy: 0.8581\n",
      "Epoch 20/50\n",
      " - 3s - loss: 0.7562 - accuracy: 0.8571 - val_loss: 0.8038 - val_accuracy: 0.8591\n",
      "Epoch 21/50\n",
      " - 2s - loss: 0.7558 - accuracy: 0.8575 - val_loss: 0.8048 - val_accuracy: 0.8594\n",
      "Epoch 22/50\n",
      " - 2s - loss: 0.7557 - accuracy: 0.8566 - val_loss: 0.8077 - val_accuracy: 0.8487\n",
      "Epoch 23/50\n",
      " - 2s - loss: 0.7540 - accuracy: 0.8584 - val_loss: 0.8052 - val_accuracy: 0.8526\n",
      "Epoch 24/50\n",
      " - 2s - loss: 0.7537 - accuracy: 0.8564 - val_loss: 0.8066 - val_accuracy: 0.8588\n",
      "Epoch 25/50\n",
      " - 2s - loss: 0.7534 - accuracy: 0.8564 - val_loss: 0.8104 - val_accuracy: 0.8565\n",
      "Epoch 26/50\n",
      " - 2s - loss: 0.7520 - accuracy: 0.8584 - val_loss: 0.8095 - val_accuracy: 0.8584\n",
      "Epoch 27/50\n",
      " - 2s - loss: 0.7526 - accuracy: 0.8577 - val_loss: 0.8079 - val_accuracy: 0.8565\n",
      "Epoch 28/50\n",
      " - 2s - loss: 0.7517 - accuracy: 0.8606 - val_loss: 0.8080 - val_accuracy: 0.8474\n",
      "Epoch 29/50\n",
      " - 3s - loss: 0.7514 - accuracy: 0.8607 - val_loss: 0.8101 - val_accuracy: 0.8549\n",
      "Epoch 30/50\n",
      " - 2s - loss: 0.7504 - accuracy: 0.8579 - val_loss: 0.8119 - val_accuracy: 0.8591\n",
      "Epoch 31/50\n",
      " - 3s - loss: 0.7502 - accuracy: 0.8582 - val_loss: 0.8115 - val_accuracy: 0.8633\n",
      "Epoch 32/50\n",
      " - 2s - loss: 0.7502 - accuracy: 0.8573 - val_loss: 0.8122 - val_accuracy: 0.8581\n",
      "Epoch 33/50\n",
      " - 2s - loss: 0.7506 - accuracy: 0.8595 - val_loss: 0.8141 - val_accuracy: 0.8594\n",
      "Epoch 34/50\n",
      " - 3s - loss: 0.7502 - accuracy: 0.8572 - val_loss: 0.8125 - val_accuracy: 0.8591\n",
      "Epoch 35/50\n",
      " - 3s - loss: 0.7491 - accuracy: 0.8595 - val_loss: 0.8103 - val_accuracy: 0.8568\n",
      "Epoch 36/50\n",
      " - 3s - loss: 0.7497 - accuracy: 0.8569 - val_loss: 0.8164 - val_accuracy: 0.8555\n",
      "Epoch 37/50\n",
      " - 3s - loss: 0.7490 - accuracy: 0.8576 - val_loss: 0.8138 - val_accuracy: 0.8545\n",
      "Epoch 38/50\n",
      " - 3s - loss: 0.7493 - accuracy: 0.8567 - val_loss: 0.8134 - val_accuracy: 0.8503\n",
      "Epoch 39/50\n",
      " - 3s - loss: 0.7491 - accuracy: 0.8594 - val_loss: 0.8117 - val_accuracy: 0.8565\n",
      "Epoch 40/50\n",
      " - 3s - loss: 0.7487 - accuracy: 0.8614 - val_loss: 0.8151 - val_accuracy: 0.8468\n",
      "Epoch 41/50\n",
      " - 3s - loss: 0.7484 - accuracy: 0.8600 - val_loss: 0.8140 - val_accuracy: 0.8568\n",
      "Epoch 42/50\n",
      " - 3s - loss: 0.7475 - accuracy: 0.8606 - val_loss: 0.8127 - val_accuracy: 0.8565\n",
      "Epoch 43/50\n",
      " - 3s - loss: 0.7477 - accuracy: 0.8592 - val_loss: 0.8153 - val_accuracy: 0.8571\n",
      "Epoch 44/50\n",
      " - 3s - loss: 0.7472 - accuracy: 0.8617 - val_loss: 0.8137 - val_accuracy: 0.8614\n",
      "Epoch 45/50\n",
      " - 3s - loss: 0.7471 - accuracy: 0.8604 - val_loss: 0.8155 - val_accuracy: 0.8568\n",
      "Epoch 46/50\n",
      " - 3s - loss: 0.7481 - accuracy: 0.8605 - val_loss: 0.8212 - val_accuracy: 0.8594\n",
      "Epoch 47/50\n",
      " - 3s - loss: 0.7470 - accuracy: 0.8622 - val_loss: 0.8183 - val_accuracy: 0.8578\n",
      "Epoch 48/50\n",
      " - 3s - loss: 0.7467 - accuracy: 0.8606 - val_loss: 0.8183 - val_accuracy: 0.8584\n",
      "Epoch 49/50\n",
      " - 3s - loss: 0.7478 - accuracy: 0.8586 - val_loss: 0.8160 - val_accuracy: 0.8558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50\n",
      " - 3s - loss: 0.7461 - accuracy: 0.8569 - val_loss: 0.8167 - val_accuracy: 0.8555\n",
      "1754/1754 [==============================] - 0s 72us/step\n",
      "Test Score: 0.81507594052579\n",
      "Test Accuracy: 0.851767361164093\n"
     ]
    }
   ],
   "source": [
    "# use categorical_crossentropy as loss function\n",
    "train_history , pred = CNN(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15400\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 150, 32)      640000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 150, 32)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 16)           3136        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          4352        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          1024        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          32896       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 7)            903         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 135)          0           dense_7[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           1360        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 7)            77          dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 716,644\n",
      "Trainable params: 716,644\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 12320 samples, validate on 3080 samples\n",
      "Epoch 1/50\n",
      " - 25s - loss: 1.0567 - accuracy: 0.7787 - val_loss: 0.8244 - val_accuracy: 0.8497\n",
      "Epoch 2/50\n",
      " - 27s - loss: 0.8241 - accuracy: 0.8309 - val_loss: 0.8053 - val_accuracy: 0.8536\n",
      "Epoch 3/50\n",
      " - 26s - loss: 0.8019 - accuracy: 0.8435 - val_loss: 0.7940 - val_accuracy: 0.8666\n",
      "Epoch 4/50\n",
      " - 26s - loss: 0.7871 - accuracy: 0.8562 - val_loss: 0.7941 - val_accuracy: 0.8575\n",
      "Epoch 5/50\n",
      " - 26s - loss: 0.7737 - accuracy: 0.8590 - val_loss: 0.7954 - val_accuracy: 0.8646\n",
      "Epoch 6/50\n",
      " - 26s - loss: 0.7623 - accuracy: 0.8640 - val_loss: 0.7838 - val_accuracy: 0.8792\n",
      "Epoch 7/50\n",
      " - 27s - loss: 0.7545 - accuracy: 0.8772 - val_loss: 0.7810 - val_accuracy: 0.8786\n",
      "Epoch 8/50\n",
      " - 26s - loss: 0.7470 - accuracy: 0.8843 - val_loss: 0.7818 - val_accuracy: 0.8760\n",
      "Epoch 9/50\n",
      " - 29s - loss: 0.7410 - accuracy: 0.8877 - val_loss: 0.7799 - val_accuracy: 0.8756\n",
      "Epoch 10/50\n",
      " - 26s - loss: 0.7349 - accuracy: 0.8898 - val_loss: 0.7833 - val_accuracy: 0.8666\n",
      "Epoch 11/50\n",
      " - 27s - loss: 0.7309 - accuracy: 0.8904 - val_loss: 0.7822 - val_accuracy: 0.8705\n",
      "Epoch 12/50\n",
      " - 27s - loss: 0.7268 - accuracy: 0.8962 - val_loss: 0.7814 - val_accuracy: 0.8610\n",
      "Epoch 13/50\n",
      " - 27s - loss: 0.7238 - accuracy: 0.8964 - val_loss: 0.7806 - val_accuracy: 0.8731\n",
      "Epoch 14/50\n",
      " - 27s - loss: 0.7196 - accuracy: 0.8985 - val_loss: 0.7837 - val_accuracy: 0.8698\n",
      "Epoch 15/50\n",
      " - 27s - loss: 0.7181 - accuracy: 0.9052 - val_loss: 0.7789 - val_accuracy: 0.8808\n",
      "Epoch 16/50\n",
      " - 26s - loss: 0.7169 - accuracy: 0.9018 - val_loss: 0.7800 - val_accuracy: 0.8705\n",
      "Epoch 17/50\n",
      " - 26s - loss: 0.7138 - accuracy: 0.9047 - val_loss: 0.7873 - val_accuracy: 0.8237\n",
      "Epoch 18/50\n",
      " - 27s - loss: 0.7121 - accuracy: 0.9090 - val_loss: 0.7821 - val_accuracy: 0.8688\n",
      "Epoch 19/50\n",
      " - 29s - loss: 0.7111 - accuracy: 0.9090 - val_loss: 0.7769 - val_accuracy: 0.8591\n",
      "Epoch 20/50\n",
      " - 27s - loss: 0.7093 - accuracy: 0.9114 - val_loss: 0.7847 - val_accuracy: 0.8581\n",
      "Epoch 21/50\n",
      " - 24s - loss: 0.7073 - accuracy: 0.9140 - val_loss: 0.7797 - val_accuracy: 0.8542\n",
      "Epoch 22/50\n",
      " - 25s - loss: 0.7059 - accuracy: 0.9148 - val_loss: 0.7842 - val_accuracy: 0.8474\n",
      "Epoch 23/50\n",
      " - 24s - loss: 0.7051 - accuracy: 0.9176 - val_loss: 0.7841 - val_accuracy: 0.8649\n",
      "Epoch 24/50\n",
      " - 26s - loss: 0.7032 - accuracy: 0.9174 - val_loss: 0.7891 - val_accuracy: 0.8545\n",
      "Epoch 25/50\n",
      " - 26s - loss: 0.7026 - accuracy: 0.9184 - val_loss: 0.7911 - val_accuracy: 0.8568\n",
      "Epoch 26/50\n",
      " - 24s - loss: 0.7010 - accuracy: 0.9218 - val_loss: 0.7883 - val_accuracy: 0.8555\n",
      "Epoch 27/50\n",
      " - 26s - loss: 0.7000 - accuracy: 0.9204 - val_loss: 0.7882 - val_accuracy: 0.8510\n",
      "Epoch 28/50\n"
     ]
    }
   ],
   "source": [
    "# use categorical_crossentropy as loss function\n",
    "train_history_ls , pred_ls = lstm(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use jsd as loss function\n",
    "train_history_jsd , pred_jsd = CNN(X_train, X_test, y_train, y_test, loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use jsd as loss function\n",
    "train_history_ls_jsd , pred_ls_jsd = lstm(X_train, X_test, y_train, y_test, loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rnss as loss function\n",
    "# train_history_rnss , pred_rnss = CNN(X_train, X_test, y_train, y_test, loss = rnss_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rnss as loss function\n",
    "# train_history_ls_rnss , pred_ls_rnss = lstm(X_train, X_test, y_train, y_test, loss = rnss_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = np.array(y_test)\n",
    "def calculate_divergence(true, pred):\n",
    "    tmp = 0\n",
    "    for i in range(len(true)):\n",
    "        tmp = tmp + jensen_shannon_div(pred[i], true[i])\n",
    "    \n",
    "    print('mean jsd :', tmp/len(true))\n",
    "    \n",
    "    tmp = 0\n",
    "    for i in range(len(true)):\n",
    "        tmp = tmp + root_normalized_squared_error(pred[i], true[i])\n",
    "    \n",
    "    print('mean rnss :', tmp/len(true))\n",
    "    \n",
    "#         print('---sentence{0}---'.format(i))\n",
    "#         print('jsd :', jensen_shannon_div(pred[i], true[i]))\n",
    "#         print('rnss :', root_normalized_squared_error(pred[i], true[i]))\n",
    "\n",
    "# print('--- textCNN ---', '\\n')\n",
    "# calculate_divergence(true, pred)\n",
    "\n",
    "# print('--- LSTM ---', '\\n')\n",
    "# calculate_divergence(true, pred_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== categorical_crossentropy ===', '\\n')\n",
    "print('---textCNN---')\n",
    "calculate_divergence(true, pred)\n",
    "print('---LSTM---')\n",
    "calculate_divergence(true, pred_ls)\n",
    "print('\\n', '=== loss_jsd ===', '\\n')\n",
    "print('---textCNN---')\n",
    "calculate_divergence(true, pred_jsd)\n",
    "print('---LSTM---')\n",
    "calculate_divergence(true, pred_ls_jsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
