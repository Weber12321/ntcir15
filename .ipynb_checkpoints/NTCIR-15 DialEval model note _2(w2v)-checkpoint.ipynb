{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Evaluation\n",
    "###### Created by Weber Huang "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "#### 0. Embedding\n",
    "#### 1. JSON to dataframe\n",
    "#### 2. Dataset cleaning\n",
    "#### 3. Segmentation\n",
    "#### 4. Generate the output of preprocessing\n",
    "#### 5. Modeling\n",
    "#### 6. Evaluation\n",
    "#### 7. Generate the submission estimation JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How to use this note?\n",
    "     \n",
    "+ In this ipython file, the model will be train autometically. Feel free to adjust the parameter of those models If you want. I'd suggest you to directly switch to section 5.       \n",
    "+ If you want to test the evaluation, here is the way:       \n",
    "    1. Run the cell  section 1 to section 4 first, and use **generate_dataset(name, wd, stop_word_path)** for trainset and devset and **test_preprocess(name, wd, stop_word_path)** for testset to process raw json to dataframe.\n",
    "    2. Run section 5 and 6.  \n",
    "    3. In section 6, there is a function **Generate_submission(dev, model_1, model_2)**, plz make sure that both 2 model are same, like both are cnn or lst.   \n",
    "    4. Save the submission copy and test it with ground_truth through eval.py     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.JSON to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nugget(dataframe):   \n",
    "    target = {'nugget'}\n",
    "    dicts = []\n",
    "    for item in tqdm(dataframe['annotations']):\n",
    "        sub_dicts = []\n",
    "        for element in item:\n",
    "            sub_dicts.append({key:value for key,value in element.items() if key in target}['nugget'])\n",
    "        dicts.append(sub_dicts)\n",
    "\n",
    "    dataframe['nuggets'] = dicts # dis is for anno nugget list\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def shaping(dataframe):\n",
    "    length = []\n",
    "    for i in tqdm(dataframe['turns']):\n",
    "        length.append(len(i))\n",
    "    Id = dataframe['id'].tolist()\n",
    "\n",
    "    Fin_Id = sum([[s] * n for s, n in zip(Id, length)], [])\n",
    "\n",
    "    turns_list = dataframe['turns'].tolist()\n",
    "    \n",
    "    Fin_turns_anno = []\n",
    "    for x,y in tqdm(zip(turns_list,dataframe['nuggets'])):\n",
    "        for q in range(len(x)):\n",
    "            Fin_turns_anno.append(list(x[q].values())+[i[q] for i in y])\n",
    "    \n",
    "    return Fin_Id, Fin_turns_anno\n",
    "\n",
    "def stacking(Fin_Id, Fin_turns_anno):    \n",
    "    train_clean = pd.DataFrame({'id': Fin_Id,'info': Fin_turns_anno})\n",
    "    # train_clean.head()\n",
    "    train_df = pd.DataFrame(train_clean['info'].values.tolist(), columns=['sender','utterance','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10','n11','n12','n13','n14','n15','n16','n17','n18','n19'])\n",
    "    train_df['id'] = train_clean['id']\n",
    "    train_df = train_df[['id','sender','utterance','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10','n11','n12','n13','n14','n15','n16','n17','n18','n19']]\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataframe):\n",
    "    \n",
    "    # id to str\n",
    "    dataframe['id'] = dataframe['id'].apply(str)\n",
    "    \n",
    "    \n",
    "    # round\n",
    "    uni = dataframe.id.unique()\n",
    "    num = []\n",
    "    for i in uni:\n",
    "        count = 1\n",
    "        for j in dataframe['id']:\n",
    "            if i == j:\n",
    "                num.append(count)\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    dataframe['round'] = num\n",
    "    \n",
    "    \n",
    "    # distribution\n",
    "    Nugget_types = ['CNUG0', 'CNUG', 'CNUG*', 'CNaN','HNUG', 'HNUG*', 'HNaN']\n",
    "    arr = np.array(dataframe.iloc[:,3:22]) \n",
    "    dicts = []\n",
    "    tmp = []\n",
    "    \n",
    "    for i in arr:\n",
    "        c = Counter(i)\n",
    "        dicts.append(c)\n",
    "        \n",
    "    for i in dicts:\n",
    "        test = []\n",
    "        for n in Nugget_types:\n",
    "            test.append(i.get(n,0)/19)\n",
    "        tmp.append(test)\n",
    "        \n",
    "    tmp = np.array(tmp)\n",
    "    for i in range(len(Nugget_types)):\n",
    "        dataframe[Nugget_types[i]] = tmp[:,i]\n",
    "        \n",
    "        \n",
    "    # round_max (round_label)\n",
    "#     f = dataframe.groupby('round').sum()\n",
    "#     out = list(f.idxmax(axis=1))\n",
    "\n",
    "#     round_max = []\n",
    "#     for i in dataframe['round']:\n",
    "#         for j in range(1,8):\n",
    "#             if i == j:\n",
    "#                 round_max.append(out[j-1])\n",
    "#             else:\n",
    "#                 continue\n",
    "#     dataframe['round_max'] = round_max\n",
    "    \n",
    "    \n",
    "    # label encoding (round_label)\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(Nugget_types);\n",
    "#     round_label = le.transform(list(dataframe['round_max']))\n",
    "#     dataframe['round_label'] = round_label\n",
    "    \n",
    "    \n",
    "    # label encoding (sender_num)\n",
    "    sender = ['customer','helpdesk']\n",
    "    l = preprocessing.LabelEncoder()\n",
    "    l.fit(sender);\n",
    "    sender_num = l.transform(list(dataframe['sender']))\n",
    "    dataframe['sender_num'] = sender_num\n",
    "    \n",
    "    subset = dataframe[['id','sender','sender_num','utterance','round',\n",
    "                        'CNUG0', 'CNUG', 'CNUG*', 'CNaN','HNUG', 'HNUG*', 'HNaN']]\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(dataframe, file_path):\n",
    "    \n",
    "    texts = dataframe['utterance'].astype(str)\n",
    "    \n",
    "    seg_texts = []\n",
    "    for line in texts:\n",
    "        seg_content = ' '.join(jieba.cut(line, cut_all = False))\n",
    "        seg_texts.append(seg_content)\n",
    "        \n",
    "    def remove_punctuation(line):\n",
    "        rule = re.compile(\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub(' ',line)\n",
    "        return line\n",
    "    \n",
    "    texts = []\n",
    "    for line in seg_texts:\n",
    "        new_line = remove_punctuation(line).split()\n",
    "        texts.append(new_line)\n",
    "        \n",
    "    cn_stopwords = []\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        for data in file.read().splitlines():\n",
    "            cn_stopwords.append(data)\n",
    "            \n",
    "    # remove punctuation\n",
    "    pp_texts = []\n",
    "    for line in texts:\n",
    "        line_noSW = []\n",
    "        for word in line:\n",
    "            if word not in cn_stopwords:\n",
    "                line_noSW.append(word)\n",
    "        pp_texts.append(line_noSW)\n",
    "    \n",
    "    # change emoji in pp_texts to *\n",
    "    for line in pp_texts:\n",
    "        if line == []:\n",
    "            line.append(\"*\")\n",
    "            \n",
    "    # concatenate the sentences by whitespace\n",
    "    new_texts = []\n",
    "    for sentence in pp_texts:\n",
    "        series_sentence = \" \".join(word for word in sentence)\n",
    "        new_texts.append(series_sentence)\n",
    "    \n",
    "    dataframe['texts'] = new_texts\n",
    "    \n",
    "    subset = dataframe[['id','sender','sender_num','texts','round',\n",
    "                        'CNUG0', 'CNUG', 'CNUG*', 'CNaN','HNUG', 'HNUG*', 'HNaN']]\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Generate the output of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from raw_json to dataframe\n",
    "def generate_dataset(name, wd, stop_word_path):\n",
    "    os.chdir(wd)\n",
    "    file = pd.read_json(name, encoding='utf8')\n",
    "    nu = get_nugget(file)\n",
    "    Id, anno = shaping(nu)\n",
    "    output = stacking(Id, anno)\n",
    "    fin = process_data(output)\n",
    "    seg = segment(fin, stop_word_path)\n",
    "    \n",
    "    return seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for test data preprocess only!\n",
    "def pre_test(dataframe):    \n",
    "    length = []\n",
    "    for i in tqdm(dataframe['turns']):\n",
    "        length.append(len(i))\n",
    "    Id = dataframe['id'].tolist()\n",
    "\n",
    "    Fin_Id = sum([[s] * n for s, n in zip(Id, length)], [])\n",
    "    turns_list = dataframe['turns'].tolist()\n",
    "    Fin_turns_anno = []\n",
    "    for x in tqdm(turns_list):\n",
    "        for q in range(len(x)):\n",
    "            Fin_turns_anno.append(list(x[q].values()))\n",
    "\n",
    "    train_clean = pd.DataFrame({'id': Fin_Id,'info': Fin_turns_anno})\n",
    "    train_df = pd.DataFrame(train_clean['info'].values.tolist(), columns=['sender','utterance'])\n",
    "    train_df['id'] = train_clean['id']\n",
    "    train_df = train_df[['id','sender','utterance']]\n",
    "\n",
    "    # id to str\n",
    "    train_df['id'] = train_df['id'].apply(str) \n",
    "    # round\n",
    "    uni = train_df.id.unique()\n",
    "    num = []\n",
    "    for i in uni:\n",
    "        count = 1\n",
    "        for j in train_df['id']:\n",
    "            if i == j:\n",
    "                num.append(count)\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    train_df['round'] = num\n",
    "    # label encoding (sender_num)\n",
    "    sender = ['customer','helpdesk']\n",
    "    l = preprocessing.LabelEncoder()\n",
    "    l.fit(sender);\n",
    "    sender_num = l.transform(list(train_df['sender']))\n",
    "    train_df['sender_num'] = sender_num\n",
    "    subset = train_df[['id','sender','sender_num','utterance','round']]\n",
    "    return subset\n",
    "\n",
    "def segment_2(dataframe, file_path):\n",
    "    texts = dataframe['utterance'].astype(str)\n",
    "    seg_texts = []\n",
    "    for line in texts:\n",
    "        seg_content = ' '.join(jieba.cut(line, cut_all = False))\n",
    "        seg_texts.append(seg_content)\n",
    "    def remove_punctuation(line):\n",
    "        rule = re.compile(\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub(' ',line)\n",
    "        return line\n",
    "    texts = []\n",
    "    for line in seg_texts:\n",
    "        new_line = remove_punctuation(line).split()\n",
    "        texts.append(new_line)  \n",
    "    cn_stopwords = []\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        for data in file.read().splitlines():\n",
    "            cn_stopwords.append(data)\n",
    "    # remove punctuation\n",
    "    pp_texts = []\n",
    "    for line in texts:\n",
    "        line_noSW = []\n",
    "        for word in line:\n",
    "            if word not in cn_stopwords:\n",
    "                line_noSW.append(word)\n",
    "        pp_texts.append(line_noSW)\n",
    "    # change emoji in pp_texts to *\n",
    "    for line in pp_texts:\n",
    "        if line == []:\n",
    "            line.append(\"*\")      \n",
    "    # concatenate the sentences by whitespace\n",
    "    new_texts = []\n",
    "    for sentence in pp_texts:\n",
    "        series_sentence = \" \".join(word for word in sentence)\n",
    "        new_texts.append(series_sentence)\n",
    "    dataframe['texts'] = new_texts\n",
    "    subset = dataframe[['id','sender','sender_num','texts','round',]]\n",
    "    return subset\n",
    "\n",
    "def test_preprocess(name, wd, stop_word_path):\n",
    "    os.chdir(wd)\n",
    "    file = pd.read_json(name, encoding='utf8')\n",
    "    t = pre_test(file)\n",
    "    tmp = segment_2(t, stop_word_path)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plz feed the raw_json, working directory and stop_word file in the generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "# import time\n",
    "# path = 'C:/Users/doudi/OneDrive/Documents/stc3-dataset/data/'\n",
    "# timestr = time.strftime(\"%Y%m%d%H%M\")\n",
    "# output.to_csv((path + timestr + '_train_data_cn.csv'), index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f545ba955e4a487297eebe3215707779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18bbb9804d540e28027c281cdd659e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aecc967c434bedb0617769baf632d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\doudi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.691 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "train = generate_dataset(r'train_cn.json',\n",
    "                          'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/New_DialEval-1',\n",
    "                         'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/DialEval-1/cn_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871ed326e4824026afff14412fbb2eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4431005f983c4e0b927cfb8eff928fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87450d6e48ed4c5f91db21a29bc9b425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "dev = generate_dataset(r'dev_cn.json',\n",
    "                          'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/New_DialEval-1',\n",
    "                         'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/DialEval-1/cn_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3eb75fff154be9a618fe625cba2df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ddca5675f54887b74fb739936320ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = test_preprocess(r'test_cn.json',\n",
    "                          'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/New_DialEval-1',\n",
    "                         'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/DialEval-1/cn_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5402a83260e74a73b2fa113f804cbd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=614083.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def import_w2v(name, wd):    \n",
    "    os.chdir(wd)\n",
    "    f = pd.read_csv(name, encoding='utf-8')\n",
    "    df = pd.DataFrame(f['614083 300'].str.split(' ',1).tolist(),\n",
    "                                     columns = ['flips','row'])\n",
    "    df = df.rename(columns={'flips':'word', 'row':'vector'})\n",
    "    \n",
    "    from opencc import OpenCC\n",
    "    s = []\n",
    "    cc = OpenCC('t2s')\n",
    "    for i in tqdm(df['word']):\n",
    "        s.append(cc.convert(i))\n",
    "    df['simp'] = s\n",
    "    \n",
    "    return df\n",
    "name = 'zh_wiki_word2vec_300.txt'\n",
    "wd = 'C:\\\\Users\\\\doudi\\\\Downloads\\\\zh_wiki_word2vec_300'\n",
    "W2V = import_w2v(name, wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "      <th>simp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>歐幾里</td>\n",
       "      <td>-0.105163544 -0.03062032 0.022532381 -0.043694...</td>\n",
       "      <td>欧几里</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>得</td>\n",
       "      <td>-0.07968008 0.3307005 -0.13564445 0.065610155 ...</td>\n",
       "      <td>得</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>西元前</td>\n",
       "      <td>0.3526686 0.1318678 -0.22770336 -0.032585304 -...</td>\n",
       "      <td>西元前</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>三世</td>\n",
       "      <td>0.20732123 0.19093925 -0.356956 0.16564949 -0....</td>\n",
       "      <td>三世</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>紀的</td>\n",
       "      <td>-0.11831374 0.6380424 -0.27801472 -0.09356173 ...</td>\n",
       "      <td>纪的</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word                                             vector simp\n",
       "0  歐幾里  -0.105163544 -0.03062032 0.022532381 -0.043694...  欧几里\n",
       "1    得  -0.07968008 0.3307005 -0.13564445 0.065610155 ...    得\n",
       "2  西元前  0.3526686 0.1318678 -0.22770336 -0.032585304 -...  西元前\n",
       "3   三世  0.20732123 0.19093925 -0.356956 0.16564949 -0....   三世\n",
       "4   紀的  -0.11831374 0.6380424 -0.27801472 -0.09356173 ...   纪的"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2V.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# transform word into vector via word2Vec\n",
    "def trans(dataframe, vec):\n",
    "    tmp_all = []\n",
    "    for i in tqdm(dataframe['texts']):\n",
    "        tmp = []\n",
    "        for j in i.split():\n",
    "            if j in vec['word'].to_list():\n",
    "#                 print(j)\n",
    "#                 print(vec[vec['word']==j]['vector'])\n",
    "                match = list(chain(*vec[vec['word']==j]['vector'].str.split().tolist()))\n",
    "                match = list(map(float, match))\n",
    "                tmp.append(match)\n",
    "#                 print(tmp)\n",
    "            else:\n",
    "                continue\n",
    "        avg = [sum(x)/len(x) for x in zip(*tmp)]\n",
    "#         print(avg)\n",
    "        tmp_all.append(avg)\n",
    "    dataframe['W2V'] = tmp_all\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2733f439bb4a4f8e829e433158f5f2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = trans(train, W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06daa9624bf8495fba363523cd102574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1755.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev = trans(dev, W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sender</th>\n",
       "      <th>sender_num</th>\n",
       "      <th>texts</th>\n",
       "      <th>round</th>\n",
       "      <th>CNUG0</th>\n",
       "      <th>CNUG</th>\n",
       "      <th>CNUG*</th>\n",
       "      <th>CNaN</th>\n",
       "      <th>HNUG</th>\n",
       "      <th>HNUG*</th>\n",
       "      <th>HNaN</th>\n",
       "      <th>W2V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3830401296796826</td>\n",
       "      <td>customer</td>\n",
       "      <td>0</td>\n",
       "      <td>中国电信 控制箱 没有 维护 信息安全 人身安全 保障 好好 修修 应该 差钱 位于 济南市...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.082410949456, 0.041056426199999996, -0.1564...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3830401296796826</td>\n",
       "      <td>helpdesk</td>\n",
       "      <td>1</td>\n",
       "      <td>您好 反映 情况 认真 记录 会 及时 相关 部门 反馈 敬请 等待</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>[-0.03553892808333333, 0.17461223858333333, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3830772740080373</td>\n",
       "      <td>customer</td>\n",
       "      <td>0</td>\n",
       "      <td>电信服务 广州市 白云区 太和 镇 谢家 庄 二队 电信 信号 差 投诉 几年 没人 处理 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.01378158932432433, 0.15494613002702703, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3830772740080373</td>\n",
       "      <td>helpdesk</td>\n",
       "      <td>1</td>\n",
       "      <td>您好 中国电信 广东 客服 关注 反映 问题 请 详细描述 一下 问题 处理 谢谢</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>[-0.030600412399999992, 0.07297016430769229, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3830772740080373</td>\n",
       "      <td>customer</td>\n",
       "      <td>0</td>\n",
       "      <td>图片 知道 起码 通信 信号 没有 更 3G 上网 信号 一年 前 两年 前 第一次 电话 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.009741488182051285, 0.19042638691025635, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id    sender  sender_num  \\\n",
       "0  3830401296796826  customer           0   \n",
       "1  3830401296796826  helpdesk           1   \n",
       "2  3830772740080373  customer           0   \n",
       "3  3830772740080373  helpdesk           1   \n",
       "4  3830772740080373  customer           0   \n",
       "\n",
       "                                               texts  round     CNUG0  \\\n",
       "0  中国电信 控制箱 没有 维护 信息安全 人身安全 保障 好好 修修 应该 差钱 位于 济南市...      1  0.736842   \n",
       "1                 您好 反映 情况 认真 记录 会 及时 相关 部门 反馈 敬请 等待      2  0.000000   \n",
       "2  电信服务 广州市 白云区 太和 镇 谢家 庄 二队 电信 信号 差 投诉 几年 没人 处理 ...      1  0.736842   \n",
       "3          您好 中国电信 广东 客服 关注 反映 问题 请 详细描述 一下 问题 处理 谢谢      2  0.000000   \n",
       "4  图片 知道 起码 通信 信号 没有 更 3G 上网 信号 一年 前 两年 前 第一次 电话 ...      3  0.052632   \n",
       "\n",
       "       CNUG  CNUG*      CNaN      HNUG     HNUG*      HNaN  \\\n",
       "0  0.052632    0.0  0.210526  0.000000  0.000000  0.000000   \n",
       "1  0.000000    0.0  0.000000  0.473684  0.000000  0.526316   \n",
       "2  0.000000    0.0  0.263158  0.000000  0.000000  0.000000   \n",
       "3  0.000000    0.0  0.000000  0.526316  0.052632  0.421053   \n",
       "4  0.736842    0.0  0.210526  0.000000  0.000000  0.000000   \n",
       "\n",
       "                                                 W2V  \n",
       "0  [0.082410949456, 0.041056426199999996, -0.1564...  \n",
       "1  [-0.03553892808333333, 0.17461223858333333, -0...  \n",
       "2  [0.01378158932432433, 0.15494613002702703, -0....  \n",
       "3  [-0.030600412399999992, 0.07297016430769229, -...  \n",
       "4  [0.009741488182051285, 0.19042638691025635, -0...  "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\train_w2v_cn.csv', index=False, encoding='utf_8_sig')\n",
    "# dev.to_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\dev_w2v_cn.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\train_w2v_cn.csv', encoding='utf_8')\n",
    "dev = pd.read_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\dev_w2v_cn.csv', encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(dataframe): \n",
    "    l = []\n",
    "    n = []\n",
    "    for i in dataframe['W2V_str']:\n",
    "        if i != '[]':\n",
    "            l.append([float(j) for j in i.split()])\n",
    "        else:\n",
    "            l.append(n)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"W2V\"] = str_to_list(train)\n",
    "dev[\"W2V\"] = str_to_list(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['W2V_str'] = str_to_list(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See if the file are same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# if output.equals(output_15_tr) == True:\n",
    "#     print(\"DialEval-14 training data is as same as DialEval-15 training data\")\n",
    "# else:\n",
    "#     print(\"DialEval-14 training data is not as same as DialEval-15 training data\")\n",
    "\n",
    "    \n",
    "# output.iloc[0] == output_15_tr.iloc[0]\n",
    "\n",
    "# # print(\"There are {0} rows in 2 datasets as same\".format(count))\n",
    "# # print(\"\\n\")\n",
    "# # print(\"There are {0} rows in 2 datasets as different\".format((len(output)-count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Write some necessory def function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(pred, truth):\n",
    "    \"\"\" convert inputs to np.array and make sure\n",
    "    inputs are normalized probability distributions\n",
    "    \"\"\"\n",
    "    if len(pred) != len(truth):\n",
    "        raise ValueError(\"pred and truth have different lengths\")\n",
    "    if len(pred) == 0 or len(truth) == 0:\n",
    "        raise ValueError(\"pred or truth are empty\")\n",
    "\n",
    "    pred, truth = np.asarray(pred), np.asarray(truth)\n",
    "    if not ((pred >= 0).all() and (truth >= 0).all()):\n",
    "        raise ValueError(\"probability distribution should not be negative\")\n",
    "    pred, truth = pred / pred.sum(), truth / truth.sum()\n",
    "    return pred, truth\n",
    "\n",
    "def jensen_shannon_div(pred, truth, base=2):\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    m = 1. / 2 * (pred + truth)\n",
    "    return (stats.entropy(pred, m, base=base)\n",
    "            + stats.entropy(truth, m, base=base)) / 2.\n",
    "\n",
    "def root_normalized_squared_error(pred, truth):\n",
    "    \"\"\" RNSS: Root Normalised Sum of Squares\n",
    "    \"\"\"\n",
    "\n",
    "    def squared_error(pred, truth):\n",
    "        return ((pred - truth) ** 2).sum()\n",
    "\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    return np.sqrt(squared_error(pred, truth) / 2)\n",
    "\n",
    "def jsd_custom_loss(y_true, y_pred):\n",
    "            \n",
    "    # calculate loss, using y_pred\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "#     y_pred, y_true = normalize(y_pred, y_true)\n",
    "    m = 1. / 2 * (y_pred + y_true)\n",
    "    # loss = (stats.entropy(y_pred, m, base=2) + stats.entropy(y_true, m, base=2)) / 2.\n",
    "    # tf.keras.losses.KLD()\n",
    "    loss = (tf.keras.losses.KLD(y_pred, m) + tf.keras.losses.KLD(y_true, m)) / 2.\n",
    "    return loss\n",
    "  \n",
    "# model.compile(loss=jsd_custom_loss, optimizer='adam')\n",
    "\n",
    "# def rnss_custom_loss(y_true, y_pred):\n",
    "            \n",
    "#     # calculate loss, using y_pred\n",
    "#     \"\"\" RNSS: Root Normalised Sum of Squares\n",
    "#     \"\"\"\n",
    "\n",
    "#     def squared_error(y_pred, y_true):\n",
    "#         return ((y_pred - y_true) ** 2).sum()\n",
    "\n",
    "# #     y_pred, y_true = normalize(y_pred, y_true)\n",
    "#     loss = np.sqrt(squared_error(y_pred, y_true) / 2)\n",
    "    \n",
    "#     return loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split from sender\n",
    "train_c = train[train.sender=='customer']\n",
    "train_h = train[train.sender=='helpdesk']\n",
    "dev_c = dev[dev.sender=='customer']\n",
    "dev_h = dev[dev.sender=='helpdesk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate, Bidirectional\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Features preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n"
     ]
    }
   ],
   "source": [
    "# === customer\n",
    "c_X_train = train_c.filter(['round','texts'])\n",
    "c_X_test = dev_c.filter(['round','texts'])\n",
    "    \n",
    "\n",
    "y_train_c = train_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "y_test_c = dev_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "\n",
    "# y_train_h = train.filter(['HNUG','HNUG*','HNaN'])\n",
    "# y_test_h = dev.filter(['HNUG','HNUG*','HNaN'])\n",
    "\n",
    "c_X1_train = c_X_train['texts']\n",
    "# c_X1_train = [str (item) for item in c_X1_train]\n",
    "c_X1_test = c_X_test['texts']\n",
    "# c_X1_test = [str (item) for item in c_X1_test]\n",
    "\n",
    "c_X2_train = c_X_train[['round']].values\n",
    "c_X2_test = c_X_test[['round']].values\n",
    "\n",
    "c_token = Tokenizer(num_words = 20000)\n",
    "c_token.fit_on_texts(c_X1_train)\n",
    "c_vocab = c_token.word_index\n",
    "print(c_token.document_count)\n",
    "\n",
    "c_x_train_seq = c_token.texts_to_sequences(c_X1_train)\n",
    "c_x_test_seq = c_token.texts_to_sequences(c_X1_test)\n",
    "c_X1_train = sequence.pad_sequences(c_x_train_seq, maxlen = 350)\n",
    "c_X1_test = sequence.pad_sequences(c_x_test_seq, maxlen = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6900\n"
     ]
    }
   ],
   "source": [
    "# === helpdesk\n",
    "h_X_train = train_h.filter(['round','texts'])\n",
    "h_X_test = dev_h.filter(['round','texts'])\n",
    "    \n",
    "\n",
    "# y_train_c = train_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "# y_test_c = dev_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "\n",
    "y_train_h = train_h.filter(['HNUG','HNUG*','HNaN'])\n",
    "y_test_h = dev_h.filter(['HNUG','HNUG*','HNaN'])\n",
    "\n",
    "h_X1_train = h_X_train['texts']\n",
    "# h_X1_train = [str (item) for item in h_X1_train]\n",
    "h_X1_test = h_X_test['texts']\n",
    "# h_X1_test = [str (item) for item in h_X1_test]\n",
    "\n",
    "h_X2_train = h_X_train[['round']].values\n",
    "h_X2_test = h_X_test[['round']].values\n",
    "\n",
    "h_token = Tokenizer(num_words = 20000)\n",
    "h_token.fit_on_texts(h_X1_train)\n",
    "h_vocab = h_token.word_index\n",
    "print(h_token.document_count)\n",
    "\n",
    "h_x_train_seq = h_token.texts_to_sequences(h_X1_train)\n",
    "h_x_test_seq = h_token.texts_to_sequences(h_X1_test)\n",
    "h_X1_train = sequence.pad_sequences(h_x_train_seq, maxlen = 350)\n",
    "h_X1_test = sequence.pad_sequences(h_x_test_seq, maxlen = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Word2Vec vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8500, 300)\n",
      "(6900, 300)\n",
      "(975, 300)\n",
      "(780, 300)\n"
     ]
    }
   ],
   "source": [
    "def to_array(dataframe, length, dim=300):   \n",
    "    train_vec = np.zeros(shape=(length,dim))\n",
    "    for i in range(len(dataframe['W2V'])):\n",
    "        if dataframe['W2V'].iloc[i] != []:\n",
    "    #         print(i)\n",
    "            train_vec[i] = dataframe['W2V'].iloc[i]\n",
    "        else:\n",
    "            continue\n",
    "    print(train_vec.shape)\n",
    "    \n",
    "    return train_vec\n",
    "\n",
    "train_vec_c = to_array(train_c, len(train_c))\n",
    "train_vec_h = to_array(train_h, len(train_h))\n",
    "dev_vec_c = to_array(dev_c, len(dev_c))\n",
    "dev_vec_h = to_array(dev_h, len(dev_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_c = np.reshape(train_vec_c, (train_vec_c.shape[0],1, train_vec_c.shape[1]))\n",
    "train_vec_h = np.reshape(train_vec_h, (train_vec_h.shape[0],1, train_vec_h.shape[1]))\n",
    "dev_vec_c = np.reshape(dev_vec_c, (dev_vec_c.shape[0],1,dev_vec_c.shape[1]))\n",
    "dev_vec_h = np.reshape(dev_vec_h, (dev_vec_h.shape[0],1,dev_vec_h.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 textCNN for customer and helpdesk respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === customer\n",
    "# def CNN_C(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):   \n",
    "    \n",
    "#     num_labels = 4\n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "\n",
    "#     sub_input = Input(shape=(2,))\n",
    "    \n",
    "#     # pre-train embeddings\n",
    "#     # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "#     # embed = embedder(main_input)\n",
    "#     embed = Embedding(len(c_vocab)+1, 300, input_length=150)(main_input)\n",
    "#     # filter size, region size\n",
    "#     cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "#     cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "#     flat = Flatten()(cnn)\n",
    "#     drop = Dropout(0.2)(flat)\n",
    "#     # main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "\n",
    "\n",
    "#     dense_1 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     drop_1 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(drop_1)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([drop, dense_2])\n",
    "#     dense_3 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=4, activation='softmax')(dense_3)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=10, \n",
    "#                               batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === helpdesk\n",
    "# def CNN_H(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "    \n",
    "#     num_labels = 2\n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "\n",
    "#     sub_input = Input(shape=(2,))\n",
    "    \n",
    "#     # pre-train embeddings\n",
    "#     # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "#     # embed = embedder(main_input)\n",
    "#     embed = Embedding(len(h_vocab)+1, 300, input_length=150)(main_input)\n",
    "#     # filter size, region size\n",
    "#     cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "#     cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "#     flat = Flatten()(cnn)\n",
    "#     drop = Dropout(0.2)(flat)\n",
    "#     # main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "\n",
    "\n",
    "#     dense_1 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     drop_1 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(drop_1)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([drop, dense_2])\n",
    "#     dense_3 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=3, activation='softmax')(dense_3)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=10, \n",
    "#                               batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 LSTM for customer and helpdesk respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === customer\n",
    "def lstm_C(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "     \n",
    "#     main_input = Input(shape=(350,), dtype='float64')\n",
    "    main_input = Input(shape=(1,300))\n",
    "    sub_input = Input(shape=(1,))\n",
    "    \n",
    "#     embed = Embedding(output_dim=300,input_dim=20000,input_length=350)(main_input)\n",
    "#     dropout_1 = Dropout(0.35)(embed)\n",
    "    lst = Bidirectional(LSTM(units=256,return_sequences=True))(main_input)\n",
    "#     lst2 = Bidirectional(LSTM(units=128))(lst)\n",
    "    merge = concatenate([lst, sub_input])\n",
    "    dense_1 = Dense(units=128,activation='relu')(merge)\n",
    "    dropout_2 = Dropout(0.35)(dense_1)\n",
    "    dense_2 = Dense(units=64,activation='relu')(dropout_2)\n",
    "    dense_3 = Dense(units=32,activation='relu')(dense_2)\n",
    "    output = Dense(units=4,activation='softmax')(dense_3)\n",
    "\n",
    "\n",
    "#     dense_4 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     dropout_3 = Dropout(0.35)(dense_4)\n",
    "# #     lst2 = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_3)\n",
    "#     dense_5 = Dense(units=128,activation='relu')(dropout_3)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([dense_3, dense_5])\n",
    "#     dense_6 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=4, activation='softmax')(dense_6)\n",
    "\n",
    "    model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "                              batch_size=128, verbose=2, validation_split=0.2)\n",
    "\n",
    "    score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "    print(\"Test Score:\", score[0])\n",
    "    print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "    pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "    return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === helpdesk\n",
    "def lstm_H(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "     \n",
    "#     main_input = Input(shape=(350,), dtype='float64')\n",
    "    main_input = Input(shape=(1,300))\n",
    "    sub_input = Input(shape=(1,))\n",
    "    \n",
    "#     embed = Embedding(output_dim=300,input_dim=20000,input_length=350)(main_input)\n",
    "#     dropout_1 = Dropout(0.35)(embed)\n",
    "    lst = Bidirectional(LSTM(units=256,return_sequences=True))(main_input)\n",
    "    lst2 = Bidirectional(LSTM(units=128))(lst)\n",
    "    merge = concatenate([lst2, sub_input])\n",
    "    dense_1 = Dense(units=64,activation='relu')(merge)\n",
    "    dropout_2 = Dropout(0.35)(dense_1)\n",
    "    dense_2 = Dense(units=32,activation='relu')(dropout_2)\n",
    "    output = Dense(units=3,activation='softmax')(dense_2)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "                              batch_size=128, verbose=2, validation_split=0.2)\n",
    "\n",
    "    score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "    print(\"Test Score:\", score[0])\n",
    "    print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "    pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "    return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Subsetting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN_c_model, CNN_c_history, CNN_c_pred = CNN_C(c_X1_train, c_X2_train, c_X1_test, \n",
    "#                                                c_X2_test, y_train_c, y_test_c, \n",
    "#                                                loss = jsd_custom_loss)\n",
    "\n",
    "# CNN_h_model, CNN_h_history, CNN_h_pred = CNN_H(h_X1_train, h_X2_train, h_X1_test, \n",
    "#                                                h_X2_test, y_train_h, y_test_h, \n",
    "#                                                loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 1, 512), (None, 1)]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-f078433480cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m lstm_c_model, lstm_c_history, lstm_c_pred = lstm_C(train_vec_c, c_X2_train, dev_vec_c, \n\u001b[0;32m      2\u001b[0m                                                \u001b[0mc_X2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_c\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                                loss = jsd_custom_loss)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-120-ce289aa77feb>\u001b[0m in \u001b[0;36mlstm_C\u001b[1;34m(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#     lst2 = Bidirectional(LSTM(units=128))(lst)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmerge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdense_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdropout_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.35\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \"\"\"\n\u001b[1;32m--> 649\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 463\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\merge.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    360\u001b[0m                              \u001b[1;34m'inputs with matching shapes '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                              \u001b[1;34m'except for the concat axis. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 1, 512), (None, 1)]"
     ]
    }
   ],
   "source": [
    "lstm_c_model, lstm_c_history, lstm_c_pred = lstm_C(train_vec_c, c_X2_train, dev_vec_c, \n",
    "                                               c_X2_test, y_train_c, y_test_c, \n",
    "                                               loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1, 300)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1, 512)       1140736     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 256)          656384      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 257)          0           bidirectional_4[0][0]            \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           16512       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 64)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 32)           2080        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 3)            99          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,815,811\n",
      "Trainable params: 1,815,811\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 5520 samples, validate on 1380 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.0521 - accuracy: 0.7998 - val_loss: 0.0354 - val_accuracy: 0.8391\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.0334 - accuracy: 0.8511 - val_loss: 0.0281 - val_accuracy: 0.8725\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.0290 - accuracy: 0.8650 - val_loss: 0.0269 - val_accuracy: 0.8746\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.0271 - accuracy: 0.8685 - val_loss: 0.0254 - val_accuracy: 0.8754\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.0255 - accuracy: 0.8716 - val_loss: 0.0246 - val_accuracy: 0.8717\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.0249 - accuracy: 0.8743 - val_loss: 0.0247 - val_accuracy: 0.8775\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.0247 - accuracy: 0.8772 - val_loss: 0.0238 - val_accuracy: 0.8775\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.0245 - accuracy: 0.8757 - val_loss: 0.0234 - val_accuracy: 0.8790\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.0237 - accuracy: 0.8799 - val_loss: 0.0233 - val_accuracy: 0.8797\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.0234 - accuracy: 0.8819 - val_loss: 0.0232 - val_accuracy: 0.8797\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.0231 - accuracy: 0.8783 - val_loss: 0.0231 - val_accuracy: 0.8775\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.0225 - accuracy: 0.8817 - val_loss: 0.0229 - val_accuracy: 0.8797\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.0222 - accuracy: 0.8772 - val_loss: 0.0235 - val_accuracy: 0.8732\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0218 - accuracy: 0.8803 - val_loss: 0.0229 - val_accuracy: 0.8768\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.0215 - accuracy: 0.8792 - val_loss: 0.0232 - val_accuracy: 0.8717\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.0214 - accuracy: 0.8808 - val_loss: 0.0229 - val_accuracy: 0.8783\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.0211 - accuracy: 0.8808 - val_loss: 0.0229 - val_accuracy: 0.8746\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.0208 - accuracy: 0.8832 - val_loss: 0.0234 - val_accuracy: 0.8739\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0207 - accuracy: 0.8822 - val_loss: 0.0239 - val_accuracy: 0.8761\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.0207 - accuracy: 0.8833 - val_loss: 0.0230 - val_accuracy: 0.8768\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.0202 - accuracy: 0.8821 - val_loss: 0.0237 - val_accuracy: 0.8717\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.0203 - accuracy: 0.8844 - val_loss: 0.0228 - val_accuracy: 0.8746\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0196 - accuracy: 0.8826 - val_loss: 0.0230 - val_accuracy: 0.8725\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.0197 - accuracy: 0.8832 - val_loss: 0.0228 - val_accuracy: 0.8754\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0193 - accuracy: 0.8857 - val_loss: 0.0234 - val_accuracy: 0.8812\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0193 - accuracy: 0.8868 - val_loss: 0.0228 - val_accuracy: 0.8761\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0189 - accuracy: 0.8851 - val_loss: 0.0230 - val_accuracy: 0.8696\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.0186 - accuracy: 0.8833 - val_loss: 0.0232 - val_accuracy: 0.8616\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0184 - accuracy: 0.8871 - val_loss: 0.0232 - val_accuracy: 0.8739\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0184 - accuracy: 0.8893 - val_loss: 0.0227 - val_accuracy: 0.8790\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.0183 - accuracy: 0.8884 - val_loss: 0.0227 - val_accuracy: 0.8696\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.0182 - accuracy: 0.8853 - val_loss: 0.0234 - val_accuracy: 0.8543\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.0183 - accuracy: 0.8841 - val_loss: 0.0227 - val_accuracy: 0.8754\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0178 - accuracy: 0.8864 - val_loss: 0.0237 - val_accuracy: 0.8790\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.0179 - accuracy: 0.8862 - val_loss: 0.0231 - val_accuracy: 0.8790\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0177 - accuracy: 0.8866 - val_loss: 0.0227 - val_accuracy: 0.8609\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.0174 - accuracy: 0.8828 - val_loss: 0.0229 - val_accuracy: 0.8797\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0175 - accuracy: 0.8873 - val_loss: 0.0231 - val_accuracy: 0.8659\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.0176 - accuracy: 0.8822 - val_loss: 0.0231 - val_accuracy: 0.8717\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.0167 - accuracy: 0.8882 - val_loss: 0.0228 - val_accuracy: 0.8667\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.0168 - accuracy: 0.8891 - val_loss: 0.0226 - val_accuracy: 0.8841\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0169 - accuracy: 0.8857 - val_loss: 0.0229 - val_accuracy: 0.8725\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0165 - accuracy: 0.8870 - val_loss: 0.0235 - val_accuracy: 0.8696\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0165 - accuracy: 0.8911 - val_loss: 0.0237 - val_accuracy: 0.8804\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.0166 - accuracy: 0.8888 - val_loss: 0.0236 - val_accuracy: 0.8775\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0165 - accuracy: 0.8862 - val_loss: 0.0233 - val_accuracy: 0.8804\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0164 - accuracy: 0.8929 - val_loss: 0.0233 - val_accuracy: 0.8652\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0158 - accuracy: 0.8942 - val_loss: 0.0230 - val_accuracy: 0.8783\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0156 - accuracy: 0.8899 - val_loss: 0.0234 - val_accuracy: 0.8768\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0158 - accuracy: 0.8893 - val_loss: 0.0229 - val_accuracy: 0.8754\n",
      "780/780 [==============================] - 0s 104us/step\n",
      "Test Score: 0.022227846525418454\n",
      "Test Accuracy: 0.8743589520454407\n"
     ]
    }
   ],
   "source": [
    "lstm_h_model, lstm_h_history, lstm_h_pred = lstm_H(train_vec_h, h_X2_train, dev_vec_h, \n",
    "                                               h_X2_test, y_train_h, y_test_h,  \n",
    "                                               loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Output by rbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict single row\n",
    "def padding_single_c(dev):\n",
    "    X_test = dev.filter(['round','texts'])\n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = [str (item) for item in X1_test]\n",
    "    X2_test = X_test[['round']].values\n",
    "    \n",
    "    x_test_seq = c_token.texts_to_sequences([X1_test])\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 350)\n",
    "    \n",
    "    return X1_test, X2_test\n",
    "\n",
    "def padding_single_h(dev):\n",
    "    X_test = dev.filter(['round','texts'])\n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = [str (item) for item in X1_test]\n",
    "    X2_test = X_test[['round']].values\n",
    "    \n",
    "    x_test_seq = h_token.texts_to_sequences([X1_test])\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 350)\n",
    "    \n",
    "    return X1_test, X2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_array_2(dataframe, length=1, dim=300):   \n",
    "    train_vec = np.zeros(shape=(length,dim))\n",
    "    if dataframe['W2V'] != []:\n",
    "        train_vec = np.array(dataframe['W2V'])\n",
    "    train_vec = train_vec.reshape(1,300)\n",
    "    \n",
    "    return np.array(train_vec)\n",
    "\n",
    "def padding_single_w2v(dev):\n",
    "    X_test = dev.filter(['round','W2V'])\n",
    "    X1_test = to_array_2(X_test)\n",
    "    X1_test = np.reshape(X1_test, (X1_test.shape[0],1,X1_test.shape[1]))\n",
    "    X2_test = X_test[['round']].values\n",
    "    return X1_test, X2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# input the development dataframe and the method\n",
    "# for current models (loss = jsd):\n",
    "# model_1 = customer model (CNN_c_model, lstm_c_model)\n",
    "# model_2 = helpdesk model (CNN_h_model, lstm_h_model)\n",
    "def Generate_submission(dev, model_1, model_2):\n",
    "    Id_list = dev['id'].unique()\n",
    "    C_nugget = ['CNUG','CNUG*','CNUG0','CNaN']\n",
    "    H_nugget = ['HNUG','HNUG*','HNaN']\n",
    "\n",
    "    final = []\n",
    "    \n",
    "    # go through each Id first\n",
    "    for Id in tqdm(Id_list):  \n",
    "        result = []\n",
    "        \n",
    "        for i in range(len(dev)):\n",
    "            \n",
    "            # if Id is match than predict the prob_distribution and zip it as dictionary \n",
    "            if dev['id'][i] == Id:\n",
    "                if dev.iloc[i, 1] == 'customer':\n",
    "                    t1, t2 = padding_single_c(dev.iloc[i])\n",
    "                    t2 = np.array(t2).reshape(1,1)\n",
    "                    cus_prob = model_1.predict(x=[t1, t2])\n",
    "                    cus_prob = cus_prob.tolist()\n",
    "                    cus_prob = list(chain(*cus_prob))\n",
    "                    dict_c = dict(zip(C_nugget, cus_prob))\n",
    "                    result.append(dict_c)\n",
    "                else:\n",
    "                    t3, t4 = padding_single_h(dev.iloc[i])\n",
    "                    t4 = np.array(t4).reshape(1,1)\n",
    "                    help_prob = model_2.predict(x=[t3, t4])\n",
    "                    help_prob = help_prob.tolist()\n",
    "                    help_prob = list(chain(*help_prob))\n",
    "                    dict_h = dict(zip(H_nugget, help_prob))\n",
    "                    result.append(dict_h)\n",
    "            # if Id isn't match than continue until it match or switch to new Id\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Submission form\n",
    "        dict1 = {'nugget':result,'id':Id}\n",
    "        final.append(dict1)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_submission_w2v(dev, model_1, model_2):\n",
    "    dev['id'] = dev['id'].apply(str)\n",
    "    Id_list = dev['id'].unique()\n",
    "    C_nugget = ['CNUG','CNUG*','CNUG0','CNaN']\n",
    "    H_nugget = ['HNUG','HNUG*','HNaN']\n",
    "\n",
    "    final = []\n",
    "    \n",
    "    # go through each Id first\n",
    "    for Id in tqdm(Id_list):  \n",
    "        result = []\n",
    "        \n",
    "        for i in range(len(dev)):\n",
    "            \n",
    "            # if Id is match than predict the prob_distribution and zip it as dictionary \n",
    "            if dev['id'][i] == Id:\n",
    "                if dev.iloc[i, 1] == 'customer':\n",
    "#                     print(i)\n",
    "                    t1, t2 = padding_single_w2v(dev.iloc[i])\n",
    "                    t2 = np.array(t2).reshape(1,1)\n",
    "                    cus_prob = model_1.predict(x=[t1, t2])\n",
    "                    cus_prob = cus_prob.tolist()\n",
    "                    cus_prob = list(chain(*cus_prob))\n",
    "                    dict_c = dict(zip(C_nugget, cus_prob))\n",
    "                    result.append(dict_c)\n",
    "                else:\n",
    "#                     print(i)\n",
    "                    t3, t4 = padding_single_w2v(dev.iloc[i])\n",
    "                    t4 = np.array(t4).reshape(1,1)\n",
    "                    help_prob = model_2.predict(x=[t3, t4])\n",
    "                    help_prob = help_prob.tolist()\n",
    "                    help_prob = list(chain(*help_prob))\n",
    "                    dict_h = dict(zip(H_nugget, help_prob))\n",
    "                    result.append(dict_h)\n",
    "            # if Id isn't match than continue until it match or switch to new Id\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Submission form\n",
    "        dict1 = {'nugget':result,'id':Id}\n",
    "        final.append(dict1)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dfaa4f8c444eee852dfb19a48f356f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# final = Generate_submission(dev, lstm_c_model, lstm_h_model)\n",
    "final = Generate_submission_w2v(dev, lstm_c_model, lstm_h_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nugget': [{'CNUG': 0.023381300270557404,\n",
       "   'CNUG*': 1.2338531632849481e-05,\n",
       "   'CNUG0': 0.23314104974269867,\n",
       "   'CNaN': 0.7434653639793396},\n",
       "  {'HNUG': 0.6867890357971191,\n",
       "   'HNUG*': 0.07672532647848129,\n",
       "   'HNaN': 0.23648561537265778},\n",
       "  {'CNUG': 0.2747816741466522,\n",
       "   'CNUG*': 2.4308390493388288e-05,\n",
       "   'CNUG0': 5.091898492537439e-05,\n",
       "   'CNaN': 0.7251431345939636},\n",
       "  {'HNUG': 0.319292277097702,\n",
       "   'HNUG*': 0.0015339427627623081,\n",
       "   'HNaN': 0.6791737675666809}],\n",
       " 'id': '4227729258237823'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate the submission estimation JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "path = 'C:/Users/doudi/OneDrive/Documents/ntcir15/eval'\n",
    "os.chdir(path)\n",
    "timestr = time.strftime(\"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open((timestr + '_' + 'dev_eval.json'), 'w', encoding='utf-8') as f: \n",
    "    f.write(json.dumps(final, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quality': None, 'nugget': {'jsd': 4.78942909465319, 'rnss': 3.0408064409102766}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\eval')\n",
    "!python eval.py 202007091616_dev_eval.json dev_cn.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036160812524601"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-4.78942909465319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12151392531225089"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-3.0408064409102766)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
