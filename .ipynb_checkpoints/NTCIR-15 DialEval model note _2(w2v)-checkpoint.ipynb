{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Evaluation\n",
    "###### Created by Weber Huang "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "#### 0. Embedding\n",
    "#### 1. JSON to dataframe\n",
    "#### 2. Dataset cleaning\n",
    "#### 3. Segmentation\n",
    "#### 4. Generate the output of preprocessing\n",
    "#### 5. Modeling\n",
    "#### 6. Evaluation\n",
    "#### 7. Generate the submission estimation JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How to use this note?\n",
    "     \n",
    "+ In this ipython file, the model will be train autometically. Feel free to adjust the parameter of those models If you want. I'd suggest you to directly switch to section 5.       \n",
    "+ If you want to test the evaluation, here is the way:       \n",
    "    1. Run the cell  section 1 to section 4 first, and use **generate_dataset(name, wd, stop_word_path)** for trainset and devset and **test_preprocess(name, wd, stop_word_path)** for testset to process raw json to dataframe.\n",
    "    2. Run section 5 and 6.  \n",
    "    3. In section 6, there is a function **Generate_submission(dev, model_1, model_2)**, plz make sure that both 2 model are same, like both are cnn or lst.   \n",
    "    4. Save the submission copy and test it with ground_truth through eval.py     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.JSON to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nugget(dataframe):   \n",
    "    target = {'nugget'}\n",
    "    dicts = []\n",
    "    for item in tqdm(dataframe['annotations']):\n",
    "        sub_dicts = []\n",
    "        for element in item:\n",
    "            sub_dicts.append({key:value for key,value in element.items() if key in target}['nugget'])\n",
    "        dicts.append(sub_dicts)\n",
    "\n",
    "    dataframe['nuggets'] = dicts # dis is for anno nugget list\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def shaping(dataframe):\n",
    "    length = []\n",
    "    for i in tqdm(dataframe['turns']):\n",
    "        length.append(len(i))\n",
    "    Id = dataframe['id'].tolist()\n",
    "\n",
    "    Fin_Id = sum([[s] * n for s, n in zip(Id, length)], [])\n",
    "\n",
    "    turns_list = dataframe['turns'].tolist()\n",
    "    \n",
    "    Fin_turns_anno = []\n",
    "    for x,y in tqdm(zip(turns_list,dataframe['nuggets'])):\n",
    "        for q in range(len(x)):\n",
    "            Fin_turns_anno.append(list(x[q].values())+[i[q] for i in y])\n",
    "    \n",
    "    return Fin_Id, Fin_turns_anno\n",
    "\n",
    "def stacking(Fin_Id, Fin_turns_anno):    \n",
    "    train_clean = pd.DataFrame({'id': Fin_Id,'info': Fin_turns_anno})\n",
    "    # train_clean.head()\n",
    "    train_df = pd.DataFrame(train_clean['info'].values.tolist(), columns=['sender','utterance','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10','n11','n12','n13','n14','n15','n16','n17','n18','n19'])\n",
    "    train_df['id'] = train_clean['id']\n",
    "    train_df = train_df[['id','sender','utterance','n1','n2','n3','n4','n5','n6','n7','n8','n9','n10','n11','n12','n13','n14','n15','n16','n17','n18','n19']]\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataframe):\n",
    "    \n",
    "    # id to str\n",
    "    dataframe['id'] = dataframe['id'].apply(str)\n",
    "    \n",
    "    \n",
    "    # round\n",
    "    uni = dataframe.id.unique()\n",
    "    num = []\n",
    "    for i in uni:\n",
    "        count = 1\n",
    "        for j in dataframe['id']:\n",
    "            if i == j:\n",
    "                num.append(count)\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    dataframe['round'] = num\n",
    "    \n",
    "    \n",
    "    # distribution\n",
    "    Nugget_types = ['CNUG0', 'CNUG', 'CNUG*', 'CNaN','HNUG', 'HNUG*', 'HNaN']\n",
    "    arr = np.array(dataframe.iloc[:,3:22]) \n",
    "    dicts = []\n",
    "    tmp = []\n",
    "    \n",
    "    for i in arr:\n",
    "        c = Counter(i)\n",
    "        dicts.append(c)\n",
    "        \n",
    "    for i in dicts:\n",
    "        test = []\n",
    "        for n in Nugget_types:\n",
    "            test.append(i.get(n,0)/19)\n",
    "        tmp.append(test)\n",
    "        \n",
    "    tmp = np.array(tmp)\n",
    "    for i in range(len(Nugget_types)):\n",
    "        dataframe[Nugget_types[i]] = tmp[:,i]\n",
    "        \n",
    "        \n",
    "    # round_max (round_label)\n",
    "#     f = dataframe.groupby('round').sum()\n",
    "#     out = list(f.idxmax(axis=1))\n",
    "\n",
    "#     round_max = []\n",
    "#     for i in dataframe['round']:\n",
    "#         for j in range(1,8):\n",
    "#             if i == j:\n",
    "#                 round_max.append(out[j-1])\n",
    "#             else:\n",
    "#                 continue\n",
    "#     dataframe['round_max'] = round_max\n",
    "    \n",
    "    \n",
    "    # label encoding (round_label)\n",
    "    \n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit(Nugget_types);\n",
    "#     round_label = le.transform(list(dataframe['round_max']))\n",
    "#     dataframe['round_label'] = round_label\n",
    "    \n",
    "    \n",
    "    # label encoding (sender_num)\n",
    "    sender = ['customer','helpdesk']\n",
    "    l = preprocessing.LabelEncoder()\n",
    "    l.fit(sender);\n",
    "    sender_num = l.transform(list(dataframe['sender']))\n",
    "    dataframe['sender_num'] = sender_num\n",
    "    \n",
    "    subset = dataframe[['id','sender','sender_num','utterance','round',\n",
    "                        'CNUG0', 'CNUG', 'CNUG*', 'CNaN','HNUG', 'HNUG*', 'HNaN']]\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(dataframe, file_path):\n",
    "    \n",
    "    texts = dataframe['utterance'].astype(str)\n",
    "    \n",
    "    seg_texts = []\n",
    "    for line in texts:\n",
    "        seg_content = ' '.join(jieba.cut(line, cut_all = False))\n",
    "        seg_texts.append(seg_content)\n",
    "        \n",
    "    def remove_punctuation(line):\n",
    "        rule = re.compile(\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub(' ',line)\n",
    "        return line\n",
    "    \n",
    "    texts = []\n",
    "    for line in seg_texts:\n",
    "        new_line = remove_punctuation(line).split()\n",
    "        texts.append(new_line)\n",
    "        \n",
    "    cn_stopwords = []\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        for data in file.read().splitlines():\n",
    "            cn_stopwords.append(data)\n",
    "            \n",
    "    # remove punctuation\n",
    "    pp_texts = []\n",
    "    for line in texts:\n",
    "        line_noSW = []\n",
    "        for word in line:\n",
    "            if word not in cn_stopwords:\n",
    "                line_noSW.append(word)\n",
    "        pp_texts.append(line_noSW)\n",
    "    \n",
    "    # change emoji in pp_texts to *\n",
    "    for line in pp_texts:\n",
    "        if line == []:\n",
    "            line.append(\"*\")\n",
    "            \n",
    "    # concatenate the sentences by whitespace\n",
    "    new_texts = []\n",
    "    for sentence in pp_texts:\n",
    "        series_sentence = \" \".join(word for word in sentence)\n",
    "        new_texts.append(series_sentence)\n",
    "    \n",
    "    dataframe['texts'] = new_texts\n",
    "    \n",
    "    subset = dataframe[['id','sender','sender_num','texts','round',\n",
    "                        'CNUG0', 'CNUG', 'CNUG*', 'CNaN','HNUG', 'HNUG*', 'HNaN']]\n",
    "    \n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Generate the output of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from raw_json to dataframe\n",
    "def generate_dataset(name, wd, stop_word_path):\n",
    "    os.chdir(wd)\n",
    "    file = pd.read_json(name, encoding='utf8')\n",
    "    nu = get_nugget(file)\n",
    "    Id, anno = shaping(nu)\n",
    "    output = stacking(Id, anno)\n",
    "    fin = process_data(output)\n",
    "    seg = segment(fin, stop_word_path)\n",
    "    \n",
    "    return seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for test data preprocess only!\n",
    "def pre_test(dataframe):    \n",
    "    length = []\n",
    "    for i in tqdm(dataframe['turns']):\n",
    "        length.append(len(i))\n",
    "    Id = dataframe['id'].tolist()\n",
    "\n",
    "    Fin_Id = sum([[s] * n for s, n in zip(Id, length)], [])\n",
    "    turns_list = dataframe['turns'].tolist()\n",
    "    Fin_turns_anno = []\n",
    "    for x in tqdm(turns_list):\n",
    "        for q in range(len(x)):\n",
    "            Fin_turns_anno.append(list(x[q].values()))\n",
    "\n",
    "    train_clean = pd.DataFrame({'id': Fin_Id,'info': Fin_turns_anno})\n",
    "    train_df = pd.DataFrame(train_clean['info'].values.tolist(), columns=['sender','utterance'])\n",
    "    train_df['id'] = train_clean['id']\n",
    "    train_df = train_df[['id','sender','utterance']]\n",
    "\n",
    "    # id to str\n",
    "    train_df['id'] = train_df['id'].apply(str) \n",
    "    # round\n",
    "    uni = train_df.id.unique()\n",
    "    num = []\n",
    "    for i in uni:\n",
    "        count = 1\n",
    "        for j in train_df['id']:\n",
    "            if i == j:\n",
    "                num.append(count)\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    train_df['round'] = num\n",
    "    # label encoding (sender_num)\n",
    "    sender = ['customer','helpdesk']\n",
    "    l = preprocessing.LabelEncoder()\n",
    "    l.fit(sender);\n",
    "    sender_num = l.transform(list(train_df['sender']))\n",
    "    train_df['sender_num'] = sender_num\n",
    "    subset = train_df[['id','sender','sender_num','utterance','round']]\n",
    "    return subset\n",
    "\n",
    "def segment_2(dataframe, file_path):\n",
    "    texts = dataframe['utterance'].astype(str)\n",
    "    seg_texts = []\n",
    "    for line in texts:\n",
    "        seg_content = ' '.join(jieba.cut(line, cut_all = False))\n",
    "        seg_texts.append(seg_content)\n",
    "    def remove_punctuation(line):\n",
    "        rule = re.compile(\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub(' ',line)\n",
    "        return line\n",
    "    texts = []\n",
    "    for line in seg_texts:\n",
    "        new_line = remove_punctuation(line).split()\n",
    "        texts.append(new_line)  \n",
    "    cn_stopwords = []\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        for data in file.read().splitlines():\n",
    "            cn_stopwords.append(data)\n",
    "    # remove punctuation\n",
    "    pp_texts = []\n",
    "    for line in texts:\n",
    "        line_noSW = []\n",
    "        for word in line:\n",
    "            if word not in cn_stopwords:\n",
    "                line_noSW.append(word)\n",
    "        pp_texts.append(line_noSW)\n",
    "    # change emoji in pp_texts to *\n",
    "    for line in pp_texts:\n",
    "        if line == []:\n",
    "            line.append(\"*\")      \n",
    "    # concatenate the sentences by whitespace\n",
    "    new_texts = []\n",
    "    for sentence in pp_texts:\n",
    "        series_sentence = \" \".join(word for word in sentence)\n",
    "        new_texts.append(series_sentence)\n",
    "    dataframe['texts'] = new_texts\n",
    "    subset = dataframe[['id','sender','sender_num','texts','round',]]\n",
    "    return subset\n",
    "\n",
    "def test_preprocess(name, wd, stop_word_path):\n",
    "    os.chdir(wd)\n",
    "    file = pd.read_json(name, encoding='utf8')\n",
    "    t = pre_test(file)\n",
    "    tmp = segment_2(t, stop_word_path)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plz feed the raw_json, working directory and stop_word file in the generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "# import time\n",
    "# path = 'C:/Users/doudi/OneDrive/Documents/stc3-dataset/data/'\n",
    "# timestr = time.strftime(\"%Y%m%d%H%M\")\n",
    "# output.to_csv((path + timestr + '_train_data_cn.csv'), index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f545ba955e4a487297eebe3215707779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18bbb9804d540e28027c281cdd659e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9aecc967c434bedb0617769baf632d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\doudi\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.691 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "train = generate_dataset(r'train_cn.json',\n",
    "                          'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/New_DialEval-1',\n",
    "                         'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/DialEval-1/cn_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871ed326e4824026afff14412fbb2eb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4431005f983c4e0b927cfb8eff928fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87450d6e48ed4c5f91db21a29bc9b425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "dev = generate_dataset(r'dev_cn.json',\n",
    "                          'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/New_DialEval-1',\n",
    "                         'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/DialEval-1/cn_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3eb75fff154be9a618fe625cba2df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ddca5675f54887b74fb739936320ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test = test_preprocess(r'test_cn.json',\n",
    "                          'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/New_DialEval-1',\n",
    "                         'C:/Users/doudi/OneDrive/Documents/ntcir15/Dataset/DialEval-1/cn_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5402a83260e74a73b2fa113f804cbd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=614083.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def import_w2v(name, wd):    \n",
    "    os.chdir(wd)\n",
    "    f = pd.read_csv(name, encoding='utf-8')\n",
    "    df = pd.DataFrame(f['614083 300'].str.split(' ',1).tolist(),\n",
    "                                     columns = ['flips','row'])\n",
    "    df = df.rename(columns={'flips':'word', 'row':'vector'})\n",
    "    \n",
    "    from opencc import OpenCC\n",
    "    s = []\n",
    "    cc = OpenCC('t2s')\n",
    "    for i in tqdm(df['word']):\n",
    "        s.append(cc.convert(i))\n",
    "    df['simp'] = s\n",
    "    \n",
    "    return df\n",
    "name = 'zh_wiki_word2vec_300.txt'\n",
    "wd = 'C:\\\\Users\\\\doudi\\\\Downloads\\\\zh_wiki_word2vec_300'\n",
    "W2V = import_w2v(name, wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "      <th>simp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>歐幾里</td>\n",
       "      <td>-0.105163544 -0.03062032 0.022532381 -0.043694...</td>\n",
       "      <td>欧几里</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>得</td>\n",
       "      <td>-0.07968008 0.3307005 -0.13564445 0.065610155 ...</td>\n",
       "      <td>得</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>西元前</td>\n",
       "      <td>0.3526686 0.1318678 -0.22770336 -0.032585304 -...</td>\n",
       "      <td>西元前</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>三世</td>\n",
       "      <td>0.20732123 0.19093925 -0.356956 0.16564949 -0....</td>\n",
       "      <td>三世</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>紀的</td>\n",
       "      <td>-0.11831374 0.6380424 -0.27801472 -0.09356173 ...</td>\n",
       "      <td>纪的</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word                                             vector simp\n",
       "0  歐幾里  -0.105163544 -0.03062032 0.022532381 -0.043694...  欧几里\n",
       "1    得  -0.07968008 0.3307005 -0.13564445 0.065610155 ...    得\n",
       "2  西元前  0.3526686 0.1318678 -0.22770336 -0.032585304 -...  西元前\n",
       "3   三世  0.20732123 0.19093925 -0.356956 0.16564949 -0....   三世\n",
       "4   紀的  -0.11831374 0.6380424 -0.27801472 -0.09356173 ...   纪的"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2V.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# transform word into vector via word2Vec\n",
    "def trans(dataframe, vec):\n",
    "    tmp_all = []\n",
    "    for i in tqdm(dataframe['texts']):\n",
    "        tmp = []\n",
    "        for j in i.split():\n",
    "            if j in vec['word'].to_list():\n",
    "#                 print(j)\n",
    "#                 print(vec[vec['word']==j]['vector'])\n",
    "                match = list(chain(*vec[vec['word']==j]['vector'].str.split().tolist()))\n",
    "                match = list(map(float, match))\n",
    "                tmp.append(match)\n",
    "#                 print(tmp)\n",
    "            else:\n",
    "                continue\n",
    "        avg = [sum(x)/len(x) for x in zip(*tmp)]\n",
    "#         print(avg)\n",
    "        tmp_all.append(avg)\n",
    "    dataframe['W2V'] = tmp_all\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2733f439bb4a4f8e829e433158f5f2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train = trans(train, W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06daa9624bf8495fba363523cd102574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1755.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev = trans(dev, W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\train_w2v_cn.csv', index=False, encoding='utf_8_sig')\n",
    "dev.to_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\dev_w2v_cn.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> See if the file are same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# if output.equals(output_15_tr) == True:\n",
    "#     print(\"DialEval-14 training data is as same as DialEval-15 training data\")\n",
    "# else:\n",
    "#     print(\"DialEval-14 training data is not as same as DialEval-15 training data\")\n",
    "\n",
    "    \n",
    "# output.iloc[0] == output_15_tr.iloc[0]\n",
    "\n",
    "# # print(\"There are {0} rows in 2 datasets as same\".format(count))\n",
    "# # print(\"\\n\")\n",
    "# # print(\"There are {0} rows in 2 datasets as different\".format((len(output)-count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Write some necessory def function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(pred, truth):\n",
    "    \"\"\" convert inputs to np.array and make sure\n",
    "    inputs are normalized probability distributions\n",
    "    \"\"\"\n",
    "    if len(pred) != len(truth):\n",
    "        raise ValueError(\"pred and truth have different lengths\")\n",
    "    if len(pred) == 0 or len(truth) == 0:\n",
    "        raise ValueError(\"pred or truth are empty\")\n",
    "\n",
    "    pred, truth = np.asarray(pred), np.asarray(truth)\n",
    "    if not ((pred >= 0).all() and (truth >= 0).all()):\n",
    "        raise ValueError(\"probability distribution should not be negative\")\n",
    "    pred, truth = pred / pred.sum(), truth / truth.sum()\n",
    "    return pred, truth\n",
    "\n",
    "def jensen_shannon_div(pred, truth, base=2):\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    m = 1. / 2 * (pred + truth)\n",
    "    return (stats.entropy(pred, m, base=base)\n",
    "            + stats.entropy(truth, m, base=base)) / 2.\n",
    "\n",
    "def root_normalized_squared_error(pred, truth):\n",
    "    \"\"\" RNSS: Root Normalised Sum of Squares\n",
    "    \"\"\"\n",
    "\n",
    "    def squared_error(pred, truth):\n",
    "        return ((pred - truth) ** 2).sum()\n",
    "\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    return np.sqrt(squared_error(pred, truth) / 2)\n",
    "\n",
    "def jsd_custom_loss(y_true, y_pred):\n",
    "            \n",
    "    # calculate loss, using y_pred\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "#     y_pred, y_true = normalize(y_pred, y_true)\n",
    "    m = 1. / 2 * (y_pred + y_true)\n",
    "    # loss = (stats.entropy(y_pred, m, base=2) + stats.entropy(y_true, m, base=2)) / 2.\n",
    "    # tf.keras.losses.KLD()\n",
    "    loss = (tf.keras.losses.KLD(y_pred, m) + tf.keras.losses.KLD(y_true, m)) / 2.\n",
    "    return loss\n",
    "  \n",
    "# model.compile(loss=jsd_custom_loss, optimizer='adam')\n",
    "\n",
    "# def rnss_custom_loss(y_true, y_pred):\n",
    "            \n",
    "#     # calculate loss, using y_pred\n",
    "#     \"\"\" RNSS: Root Normalised Sum of Squares\n",
    "#     \"\"\"\n",
    "\n",
    "#     def squared_error(y_pred, y_true):\n",
    "#         return ((y_pred - y_true) ** 2).sum()\n",
    "\n",
    "# #     y_pred, y_true = normalize(y_pred, y_true)\n",
    "#     loss = np.sqrt(squared_error(y_pred, y_true) / 2)\n",
    "    \n",
    "#     return loss\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split from sender\n",
    "train_c = train[train.sender=='customer']\n",
    "train_h = train[train.sender=='helpdesk']\n",
    "dev_c = dev[dev.sender=='customer']\n",
    "dev_h = dev[dev.sender=='helpdesk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate, Bidirectional\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Features preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n"
     ]
    }
   ],
   "source": [
    "# === customer\n",
    "c_X_train = train_c.filter(['round','texts'])\n",
    "c_X_test = dev_c.filter(['round','texts'])\n",
    "    \n",
    "\n",
    "y_train_c = train_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "y_test_c = dev_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "\n",
    "# y_train_h = train.filter(['HNUG','HNUG*','HNaN'])\n",
    "# y_test_h = dev.filter(['HNUG','HNUG*','HNaN'])\n",
    "\n",
    "c_X1_train = c_X_train['texts']\n",
    "# c_X1_train = [str (item) for item in c_X1_train]\n",
    "c_X1_test = c_X_test['texts']\n",
    "# c_X1_test = [str (item) for item in c_X1_test]\n",
    "\n",
    "c_X2_train = c_X_train[['round']].values\n",
    "c_X2_test = c_X_test[['round']].values\n",
    "\n",
    "c_token = Tokenizer(num_words = 20000)\n",
    "c_token.fit_on_texts(c_X1_train)\n",
    "c_vocab = c_token.word_index\n",
    "print(c_token.document_count)\n",
    "\n",
    "c_x_train_seq = c_token.texts_to_sequences(c_X1_train)\n",
    "c_x_test_seq = c_token.texts_to_sequences(c_X1_test)\n",
    "c_X1_train = sequence.pad_sequences(c_x_train_seq, maxlen = 350)\n",
    "c_X1_test = sequence.pad_sequences(c_x_test_seq, maxlen = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6900\n"
     ]
    }
   ],
   "source": [
    "# === helpdesk\n",
    "h_X_train = train_h.filter(['round','texts'])\n",
    "h_X_test = dev_h.filter(['round','texts'])\n",
    "    \n",
    "\n",
    "# y_train_c = train_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "# y_test_c = dev_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "\n",
    "y_train_h = train_h.filter(['HNUG','HNUG*','HNaN'])\n",
    "y_test_h = dev_h.filter(['HNUG','HNUG*','HNaN'])\n",
    "\n",
    "h_X1_train = h_X_train['texts']\n",
    "# h_X1_train = [str (item) for item in h_X1_train]\n",
    "h_X1_test = h_X_test['texts']\n",
    "# h_X1_test = [str (item) for item in h_X1_test]\n",
    "\n",
    "h_X2_train = h_X_train[['round']].values\n",
    "h_X2_test = h_X_test[['round']].values\n",
    "\n",
    "h_token = Tokenizer(num_words = 20000)\n",
    "h_token.fit_on_texts(h_X1_train)\n",
    "h_vocab = h_token.word_index\n",
    "print(h_token.document_count)\n",
    "\n",
    "h_x_train_seq = h_token.texts_to_sequences(h_X1_train)\n",
    "h_x_test_seq = h_token.texts_to_sequences(h_X1_test)\n",
    "h_X1_train = sequence.pad_sequences(h_x_train_seq, maxlen = 350)\n",
    "h_X1_test = sequence.pad_sequences(h_x_test_seq, maxlen = 350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 textCNN for customer and helpdesk respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === customer\n",
    "# def CNN_C(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):   \n",
    "    \n",
    "#     num_labels = 4\n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "\n",
    "#     sub_input = Input(shape=(2,))\n",
    "    \n",
    "#     # pre-train embeddings\n",
    "#     # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "#     # embed = embedder(main_input)\n",
    "#     embed = Embedding(len(c_vocab)+1, 300, input_length=150)(main_input)\n",
    "#     # filter size, region size\n",
    "#     cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "#     cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "#     flat = Flatten()(cnn)\n",
    "#     drop = Dropout(0.2)(flat)\n",
    "#     # main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "\n",
    "\n",
    "#     dense_1 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     drop_1 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(drop_1)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([drop, dense_2])\n",
    "#     dense_3 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=4, activation='softmax')(dense_3)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=10, \n",
    "#                               batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === helpdesk\n",
    "# def CNN_H(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "    \n",
    "#     num_labels = 2\n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "\n",
    "#     sub_input = Input(shape=(2,))\n",
    "    \n",
    "#     # pre-train embeddings\n",
    "#     # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "#     # embed = embedder(main_input)\n",
    "#     embed = Embedding(len(h_vocab)+1, 300, input_length=150)(main_input)\n",
    "#     # filter size, region size\n",
    "#     cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "#     cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "#     flat = Flatten()(cnn)\n",
    "#     drop = Dropout(0.2)(flat)\n",
    "#     # main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "\n",
    "\n",
    "#     dense_1 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     drop_1 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(drop_1)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([drop, dense_2])\n",
    "#     dense_3 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=3, activation='softmax')(dense_3)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=10, \n",
    "#                               batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 LSTM for customer and helpdesk respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === customer\n",
    "def lstm_C(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "     \n",
    "    main_input = Input(shape=(350,), dtype='float64')\n",
    "    sub_input = Input(shape=(1,))\n",
    "    \n",
    "    embed = Embedding(output_dim=300,input_dim=20000,input_length=350)(main_input)\n",
    "    dropout_1 = Dropout(0.35)(embed)\n",
    "    lst = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_1)\n",
    "    lst2 = Bidirectional(LSTM(units=8))(lst)\n",
    "    dense_1 = Dense(units=256,activation='relu')(lst2)\n",
    "    dropout_2 = Dropout(0.35)(dense_1)\n",
    "    dense_2 = Dense(units=128,activation='relu')(dropout_2)\n",
    "    dense_3 = Dense(units=4,activation='softmax')(dense_2)\n",
    "\n",
    "\n",
    "    dense_4 = Dense(units=256,activation='relu')(sub_input)\n",
    "    dropout_3 = Dropout(0.35)(dense_4)\n",
    "#     lst2 = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_3)\n",
    "    dense_5 = Dense(units=128,activation='relu')(dropout_3)\n",
    "    # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "    merge = concatenate([dense_3, dense_5])\n",
    "    dense_6 = Dense(units=10, activation='relu')(merge)\n",
    "    output = Dense(units=4, activation='softmax')(dense_6)\n",
    "\n",
    "    model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "                              batch_size=128, verbose=2, validation_split=0.2)\n",
    "\n",
    "    score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "    print(\"Test Score:\", score[0])\n",
    "    print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "    pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "    return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === helpdesk\n",
    "def lstm_H(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "     \n",
    "    main_input = Input(shape=(350,), dtype='float64')\n",
    "    sub_input = Input(shape=(1,))\n",
    "    \n",
    "    embed = Embedding(output_dim=300,input_dim=20000,input_length=350)(main_input)\n",
    "    dropout_1 = Dropout(0.35)(embed)\n",
    "    lst = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_1)\n",
    "    lst2 = Bidirectional(LSTM(units=8))(lst)\n",
    "    dense_1 = Dense(units=256,activation='relu')(lst2)\n",
    "    dropout_2 = Dropout(0.35)(dense_1)\n",
    "    dense_2 = Dense(units=128,activation='relu')(dropout_2)\n",
    "    dense_3 = Dense(units=3,activation='softmax')(dense_2)\n",
    "\n",
    "\n",
    "    dense_4 = Dense(units=256,activation='relu')(sub_input)\n",
    "    dropout_3 = Dropout(0.35)(dense_4)\n",
    "#     lst2 = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_3)\n",
    "    dense_5 = Dense(units=128,activation='relu')(dropout_3)\n",
    "    # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "    merge = concatenate([dense_3, dense_5])\n",
    "    dense_6 = Dense(units=10, activation='relu')(merge)\n",
    "    output = Dense(units=3, activation='softmax')(dense_6)\n",
    "\n",
    "    model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "\n",
    "    # checkpoint\n",
    "    # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "    # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "    train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "                              batch_size=128, verbose=2, validation_split=0.2)\n",
    "\n",
    "    score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "    print(\"Test Score:\", score[0])\n",
    "    print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "    pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "    predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "    return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Subsetting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN_c_model, CNN_c_history, CNN_c_pred = CNN_C(c_X1_train, c_X2_train, c_X1_test, \n",
    "#                                                c_X2_test, y_train_c, y_test_c, \n",
    "#                                                loss = jsd_custom_loss)\n",
    "\n",
    "# CNN_h_model, CNN_h_history, CNN_h_pred = CNN_H(h_X1_train, h_X2_train, h_X1_test, \n",
    "#                                                h_X2_test, y_train_h, y_test_h, \n",
    "#                                                loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 350)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 350, 300)     6000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 350, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 350, 32)      40576       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 16)           2624        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          4352        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 4)            516         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 132)          0           dense_3[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 10)           1330        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 4)            44          dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,115,746\n",
      "Trainable params: 6,115,746\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/50\n",
      " - 49s - loss: 0.1477 - accuracy: 0.4810 - val_loss: 0.1138 - val_accuracy: 0.8747\n",
      "Epoch 2/50\n",
      " - 47s - loss: 0.0904 - accuracy: 0.8515 - val_loss: 0.0551 - val_accuracy: 0.8747\n",
      "Epoch 3/50\n",
      " - 47s - loss: 0.0567 - accuracy: 0.8500 - val_loss: 0.0491 - val_accuracy: 0.8747\n",
      "Epoch 4/50\n",
      " - 47s - loss: 0.0516 - accuracy: 0.8526 - val_loss: 0.0457 - val_accuracy: 0.8747\n",
      "Epoch 5/50\n",
      " - 47s - loss: 0.0477 - accuracy: 0.8515 - val_loss: 0.0415 - val_accuracy: 0.8747\n",
      "Epoch 6/50\n",
      " - 47s - loss: 0.0440 - accuracy: 0.8503 - val_loss: 0.0398 - val_accuracy: 0.8747\n",
      "Epoch 7/50\n",
      " - 47s - loss: 0.0413 - accuracy: 0.8510 - val_loss: 0.0389 - val_accuracy: 0.8747\n",
      "Epoch 8/50\n",
      " - 47s - loss: 0.0398 - accuracy: 0.8504 - val_loss: 0.0390 - val_accuracy: 0.8788\n",
      "Epoch 9/50\n",
      " - 47s - loss: 0.0385 - accuracy: 0.8500 - val_loss: 0.0389 - val_accuracy: 0.8771\n",
      "Epoch 10/50\n",
      " - 47s - loss: 0.0372 - accuracy: 0.8487 - val_loss: 0.0388 - val_accuracy: 0.8788\n",
      "Epoch 11/50\n",
      " - 47s - loss: 0.0357 - accuracy: 0.8518 - val_loss: 0.0390 - val_accuracy: 0.8788\n",
      "Epoch 12/50\n",
      " - 47s - loss: 0.0346 - accuracy: 0.8560 - val_loss: 0.0383 - val_accuracy: 0.8788\n",
      "Epoch 13/50\n",
      " - 47s - loss: 0.0335 - accuracy: 0.8584 - val_loss: 0.0380 - val_accuracy: 0.8747\n",
      "Epoch 14/50\n",
      " - 47s - loss: 0.0326 - accuracy: 0.8613 - val_loss: 0.0391 - val_accuracy: 0.8576\n",
      "Epoch 15/50\n",
      " - 48s - loss: 0.0317 - accuracy: 0.8703 - val_loss: 0.0376 - val_accuracy: 0.8729\n",
      "Epoch 16/50\n",
      " - 47s - loss: 0.0309 - accuracy: 0.8856 - val_loss: 0.0381 - val_accuracy: 0.8624\n",
      "Epoch 17/50\n",
      " - 47s - loss: 0.0304 - accuracy: 0.8953 - val_loss: 0.0380 - val_accuracy: 0.8729\n",
      "Epoch 18/50\n",
      " - 48s - loss: 0.0298 - accuracy: 0.8982 - val_loss: 0.0380 - val_accuracy: 0.8771\n",
      "Epoch 19/50\n",
      " - 47s - loss: 0.0291 - accuracy: 0.9076 - val_loss: 0.0382 - val_accuracy: 0.8753\n",
      "Epoch 20/50\n",
      " - 47s - loss: 0.0287 - accuracy: 0.9126 - val_loss: 0.0382 - val_accuracy: 0.8753\n",
      "Epoch 21/50\n",
      " - 47s - loss: 0.0285 - accuracy: 0.9118 - val_loss: 0.0384 - val_accuracy: 0.8735\n",
      "Epoch 22/50\n",
      " - 47s - loss: 0.0281 - accuracy: 0.9166 - val_loss: 0.0394 - val_accuracy: 0.8612\n",
      "Epoch 23/50\n",
      " - 47s - loss: 0.0282 - accuracy: 0.9121 - val_loss: 0.0384 - val_accuracy: 0.8694\n",
      "Epoch 24/50\n",
      " - 47s - loss: 0.0277 - accuracy: 0.9137 - val_loss: 0.0386 - val_accuracy: 0.8676\n",
      "Epoch 25/50\n",
      " - 48s - loss: 0.0274 - accuracy: 0.9187 - val_loss: 0.0390 - val_accuracy: 0.8671\n",
      "Epoch 26/50\n",
      " - 47s - loss: 0.0271 - accuracy: 0.9196 - val_loss: 0.0387 - val_accuracy: 0.8718\n",
      "Epoch 27/50\n",
      " - 47s - loss: 0.0270 - accuracy: 0.9200 - val_loss: 0.0384 - val_accuracy: 0.8735\n",
      "Epoch 28/50\n",
      " - 47s - loss: 0.0271 - accuracy: 0.9162 - val_loss: 0.0387 - val_accuracy: 0.8676\n",
      "Epoch 29/50\n",
      " - 47s - loss: 0.0268 - accuracy: 0.9212 - val_loss: 0.0386 - val_accuracy: 0.8759\n",
      "Epoch 30/50\n",
      " - 47s - loss: 0.0267 - accuracy: 0.9226 - val_loss: 0.0387 - val_accuracy: 0.8735\n",
      "Epoch 31/50\n",
      " - 48s - loss: 0.0266 - accuracy: 0.9268 - val_loss: 0.0385 - val_accuracy: 0.8671\n",
      "Epoch 32/50\n",
      " - 47s - loss: 0.0265 - accuracy: 0.9254 - val_loss: 0.0391 - val_accuracy: 0.8641\n",
      "Epoch 33/50\n",
      " - 47s - loss: 0.0264 - accuracy: 0.9266 - val_loss: 0.0387 - val_accuracy: 0.8706\n",
      "Epoch 34/50\n",
      " - 48s - loss: 0.0263 - accuracy: 0.9275 - val_loss: 0.0386 - val_accuracy: 0.8759\n",
      "Epoch 35/50\n",
      " - 47s - loss: 0.0264 - accuracy: 0.9218 - val_loss: 0.0389 - val_accuracy: 0.8688\n",
      "Epoch 36/50\n",
      " - 47s - loss: 0.0263 - accuracy: 0.9272 - val_loss: 0.0387 - val_accuracy: 0.8694\n",
      "Epoch 37/50\n",
      " - 47s - loss: 0.0263 - accuracy: 0.9249 - val_loss: 0.0392 - val_accuracy: 0.8765\n",
      "Epoch 38/50\n",
      " - 48s - loss: 0.0262 - accuracy: 0.9268 - val_loss: 0.0391 - val_accuracy: 0.8624\n",
      "Epoch 39/50\n",
      " - 47s - loss: 0.0263 - accuracy: 0.9260 - val_loss: 0.0393 - val_accuracy: 0.8782\n",
      "Epoch 40/50\n",
      " - 47s - loss: 0.0262 - accuracy: 0.9260 - val_loss: 0.0388 - val_accuracy: 0.8724\n",
      "Epoch 41/50\n",
      " - 47s - loss: 0.0262 - accuracy: 0.9260 - val_loss: 0.0389 - val_accuracy: 0.8718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50\n",
      " - 47s - loss: 0.0261 - accuracy: 0.9259 - val_loss: 0.0395 - val_accuracy: 0.8665\n",
      "Epoch 43/50\n",
      " - 47s - loss: 0.0261 - accuracy: 0.9272 - val_loss: 0.0390 - val_accuracy: 0.8724\n",
      "Epoch 44/50\n",
      " - 47s - loss: 0.0260 - accuracy: 0.9278 - val_loss: 0.0393 - val_accuracy: 0.8759\n",
      "Epoch 45/50\n",
      " - 48s - loss: 0.0261 - accuracy: 0.9276 - val_loss: 0.0394 - val_accuracy: 0.8641\n",
      "Epoch 46/50\n",
      " - 47s - loss: 0.0260 - accuracy: 0.9268 - val_loss: 0.0389 - val_accuracy: 0.8706\n",
      "Epoch 47/50\n",
      " - 47s - loss: 0.0260 - accuracy: 0.9300 - val_loss: 0.0392 - val_accuracy: 0.8688\n",
      "Epoch 48/50\n",
      " - 47s - loss: 0.0261 - accuracy: 0.9296 - val_loss: 0.0396 - val_accuracy: 0.8665\n",
      "Epoch 49/50\n",
      " - 47s - loss: 0.0260 - accuracy: 0.9278 - val_loss: 0.0395 - val_accuracy: 0.8624\n",
      "Epoch 50/50\n",
      " - 47s - loss: 0.0259 - accuracy: 0.9294 - val_loss: 0.0392 - val_accuracy: 0.8724\n",
      "975/975 [==============================] - 10s 10ms/step\n",
      "Test Score: 0.03645758453661051\n",
      "Test Accuracy: 0.854358971118927\n"
     ]
    }
   ],
   "source": [
    "lstm_c_model, lstm_c_history, lstm_c_pred = lstm_C(c_X1_train, c_X2_train, c_X1_test, \n",
    "                                               c_X2_test, y_train_c, y_test_c, \n",
    "                                               loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 350)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 350, 300)     6000000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 350, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 350, 32)      40576       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 16)           2624        bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          4352        bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256)          0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 256)          512         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 128)          32896       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 3)            387         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 128)          32896       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 131)          0           dense_10[0][0]                   \n",
      "                                                                 dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 10)           1320        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 3)            33          dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,115,596\n",
      "Trainable params: 6,115,596\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 5520 samples, validate on 1380 samples\n",
      "Epoch 1/50\n",
      " - 39s - loss: 0.0557 - accuracy: 0.8085 - val_loss: 0.0507 - val_accuracy: 0.8217\n",
      "Epoch 2/50\n",
      " - 38s - loss: 0.0485 - accuracy: 0.8141 - val_loss: 0.0481 - val_accuracy: 0.8217\n",
      "Epoch 3/50\n",
      " - 38s - loss: 0.0465 - accuracy: 0.8121 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 4/50\n",
      " - 38s - loss: 0.0460 - accuracy: 0.8149 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 5/50\n",
      " - 37s - loss: 0.0458 - accuracy: 0.8145 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 6/50\n",
      " - 38s - loss: 0.0458 - accuracy: 0.8145 - val_loss: 0.0457 - val_accuracy: 0.8217\n",
      "Epoch 7/50\n",
      " - 37s - loss: 0.0432 - accuracy: 0.8141 - val_loss: 0.0415 - val_accuracy: 0.8217\n",
      "Epoch 8/50\n",
      " - 38s - loss: 0.0373 - accuracy: 0.8163 - val_loss: 0.0341 - val_accuracy: 0.8217\n",
      "Epoch 9/50\n",
      " - 38s - loss: 0.0312 - accuracy: 0.8163 - val_loss: 0.0300 - val_accuracy: 0.8217\n",
      "Epoch 10/50\n",
      " - 37s - loss: 0.0275 - accuracy: 0.8178 - val_loss: 0.0286 - val_accuracy: 0.8348\n",
      "Epoch 11/50\n",
      " - 38s - loss: 0.0247 - accuracy: 0.8223 - val_loss: 0.0275 - val_accuracy: 0.8536\n",
      "Epoch 12/50\n",
      " - 38s - loss: 0.0223 - accuracy: 0.8379 - val_loss: 0.0269 - val_accuracy: 0.8681\n",
      "Epoch 13/50\n",
      " - 38s - loss: 0.0199 - accuracy: 0.8670 - val_loss: 0.0248 - val_accuracy: 0.8630\n",
      "Epoch 14/50\n",
      " - 38s - loss: 0.0177 - accuracy: 0.8752 - val_loss: 0.0245 - val_accuracy: 0.8870\n",
      "Epoch 15/50\n",
      " - 38s - loss: 0.0161 - accuracy: 0.8853 - val_loss: 0.0232 - val_accuracy: 0.8833\n",
      "Epoch 16/50\n",
      " - 38s - loss: 0.0147 - accuracy: 0.8886 - val_loss: 0.0232 - val_accuracy: 0.8848\n",
      "Epoch 17/50\n",
      " - 38s - loss: 0.0138 - accuracy: 0.8870 - val_loss: 0.0232 - val_accuracy: 0.8493\n",
      "Epoch 18/50\n",
      " - 38s - loss: 0.0129 - accuracy: 0.8966 - val_loss: 0.0232 - val_accuracy: 0.8572\n",
      "Epoch 19/50\n",
      " - 38s - loss: 0.0122 - accuracy: 0.9000 - val_loss: 0.0237 - val_accuracy: 0.8428\n",
      "Epoch 20/50\n",
      " - 38s - loss: 0.0116 - accuracy: 0.9060 - val_loss: 0.0236 - val_accuracy: 0.8507\n",
      "Epoch 21/50\n",
      " - 38s - loss: 0.0112 - accuracy: 0.9111 - val_loss: 0.0241 - val_accuracy: 0.8268\n",
      "Epoch 22/50\n",
      " - 38s - loss: 0.0107 - accuracy: 0.9132 - val_loss: 0.0245 - val_accuracy: 0.8355\n",
      "Epoch 23/50\n",
      " - 38s - loss: 0.0103 - accuracy: 0.9181 - val_loss: 0.0238 - val_accuracy: 0.8529\n",
      "Epoch 24/50\n",
      " - 38s - loss: 0.0099 - accuracy: 0.9214 - val_loss: 0.0244 - val_accuracy: 0.8486\n",
      "Epoch 25/50\n",
      " - 38s - loss: 0.0096 - accuracy: 0.9210 - val_loss: 0.0240 - val_accuracy: 0.8638\n",
      "Epoch 26/50\n",
      " - 38s - loss: 0.0094 - accuracy: 0.9277 - val_loss: 0.0237 - val_accuracy: 0.8609\n",
      "Epoch 27/50\n",
      " - 38s - loss: 0.0093 - accuracy: 0.9217 - val_loss: 0.0241 - val_accuracy: 0.8587\n",
      "Epoch 28/50\n",
      " - 38s - loss: 0.0090 - accuracy: 0.9221 - val_loss: 0.0245 - val_accuracy: 0.8507\n",
      "Epoch 29/50\n",
      " - 38s - loss: 0.0087 - accuracy: 0.9279 - val_loss: 0.0238 - val_accuracy: 0.8717\n",
      "Epoch 30/50\n",
      " - 38s - loss: 0.0085 - accuracy: 0.9290 - val_loss: 0.0245 - val_accuracy: 0.8609\n",
      "Epoch 31/50\n",
      " - 38s - loss: 0.0084 - accuracy: 0.9295 - val_loss: 0.0245 - val_accuracy: 0.8623\n",
      "Epoch 32/50\n",
      " - 38s - loss: 0.0082 - accuracy: 0.9310 - val_loss: 0.0239 - val_accuracy: 0.8681\n",
      "Epoch 33/50\n",
      " - 38s - loss: 0.0078 - accuracy: 0.9333 - val_loss: 0.0246 - val_accuracy: 0.8623\n",
      "Epoch 34/50\n",
      " - 38s - loss: 0.0078 - accuracy: 0.9337 - val_loss: 0.0246 - val_accuracy: 0.8616\n",
      "Epoch 35/50\n",
      " - 38s - loss: 0.0076 - accuracy: 0.9351 - val_loss: 0.0244 - val_accuracy: 0.8659\n",
      "Epoch 36/50\n",
      " - 38s - loss: 0.0074 - accuracy: 0.9359 - val_loss: 0.0256 - val_accuracy: 0.8522\n",
      "Epoch 37/50\n",
      " - 38s - loss: 0.0073 - accuracy: 0.9326 - val_loss: 0.0250 - val_accuracy: 0.8543\n",
      "Epoch 38/50\n",
      " - 38s - loss: 0.0072 - accuracy: 0.9355 - val_loss: 0.0249 - val_accuracy: 0.8616\n",
      "Epoch 39/50\n",
      " - 38s - loss: 0.0070 - accuracy: 0.9379 - val_loss: 0.0249 - val_accuracy: 0.8681\n",
      "Epoch 40/50\n",
      " - 38s - loss: 0.0070 - accuracy: 0.9350 - val_loss: 0.0252 - val_accuracy: 0.8630\n",
      "Epoch 41/50\n",
      " - 38s - loss: 0.0068 - accuracy: 0.9361 - val_loss: 0.0251 - val_accuracy: 0.8609\n",
      "Epoch 42/50\n",
      " - 38s - loss: 0.0068 - accuracy: 0.9375 - val_loss: 0.0252 - val_accuracy: 0.8601\n",
      "Epoch 43/50\n",
      " - 38s - loss: 0.0066 - accuracy: 0.9397 - val_loss: 0.0246 - val_accuracy: 0.8739\n",
      "Epoch 44/50\n",
      " - 38s - loss: 0.0066 - accuracy: 0.9397 - val_loss: 0.0255 - val_accuracy: 0.8609\n",
      "Epoch 45/50\n",
      " - 38s - loss: 0.0063 - accuracy: 0.9415 - val_loss: 0.0250 - val_accuracy: 0.8710\n",
      "Epoch 46/50\n",
      " - 38s - loss: 0.0064 - accuracy: 0.9386 - val_loss: 0.0252 - val_accuracy: 0.8696\n",
      "Epoch 47/50\n",
      " - 38s - loss: 0.0063 - accuracy: 0.9411 - val_loss: 0.0262 - val_accuracy: 0.8551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      " - 38s - loss: 0.0062 - accuracy: 0.9399 - val_loss: 0.0248 - val_accuracy: 0.8696\n",
      "Epoch 49/50\n",
      " - 38s - loss: 0.0060 - accuracy: 0.9380 - val_loss: 0.0256 - val_accuracy: 0.8587\n",
      "Epoch 50/50\n",
      " - 38s - loss: 0.0059 - accuracy: 0.9411 - val_loss: 0.0258 - val_accuracy: 0.8667\n",
      "780/780 [==============================] - 8s 10ms/step\n",
      "Test Score: 0.025325430103410512\n",
      "Test Accuracy: 0.8461538553237915\n"
     ]
    }
   ],
   "source": [
    "lstm_h_model, lstm_h_history, lstm_h_pred = lstm_H(h_X1_train, h_X2_train, h_X1_test, \n",
    "                                               h_X2_test, y_train_h, y_test_h,  \n",
    "                                               loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Output by rbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict single row\n",
    "def padding_single_c(dev):\n",
    "    X_test = dev.filter(['round','texts'])\n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = [str (item) for item in X1_test]\n",
    "    X2_test = X_test[['round']].values\n",
    "    \n",
    "    x_test_seq = c_token.texts_to_sequences([X1_test])\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 350)\n",
    "    \n",
    "    return X1_test, X2_test\n",
    "\n",
    "def padding_single_h(dev):\n",
    "    X_test = dev.filter(['round','texts'])\n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = [str (item) for item in X1_test]\n",
    "    X2_test = X_test[['round']].values\n",
    "    \n",
    "    x_test_seq = h_token.texts_to_sequences([X1_test])\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 350)\n",
    "    \n",
    "    return X1_test, X2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# input the development dataframe and the method\n",
    "# for current models (loss = jsd):\n",
    "# model_1 = customer model (CNN_c_model, lstm_c_model)\n",
    "# model_2 = helpdesk model (CNN_h_model, lstm_h_model)\n",
    "def Generate_submission(dev, model_1, model_2):\n",
    "    Id_list = dev['id'].unique()\n",
    "    C_nugget = ['CNUG','CNUG*','CNUG0','CNaN']\n",
    "    H_nugget = ['HNUG','HNUG*','HNaN']\n",
    "\n",
    "    final = []\n",
    "    \n",
    "    # go through each Id first\n",
    "    for Id in tqdm(Id_list):  \n",
    "        result = []\n",
    "        \n",
    "        for i in range(len(dev)):\n",
    "            \n",
    "            # if Id is match than predict the prob_distribution and zip it as dictionary \n",
    "            if dev['id'][i] == Id:\n",
    "                if dev.iloc[i, 1] == 'customer':\n",
    "                    t1, t2 = padding_single_c(dev.iloc[i])\n",
    "                    t2 = np.array(t2).reshape(1,1)\n",
    "                    cus_prob = model_1.predict(x=[t1, t2])\n",
    "                    cus_prob = cus_prob.tolist()\n",
    "                    cus_prob = list(chain(*cus_prob))\n",
    "                    dict_c = dict(zip(C_nugget, cus_prob))\n",
    "                    result.append(dict_c)\n",
    "                else:\n",
    "                    t3, t4 = padding_single_h(dev.iloc[i])\n",
    "                    t4 = np.array(t4).reshape(1,1)\n",
    "                    help_prob = model_2.predict(x=[t3, t4])\n",
    "                    help_prob = help_prob.tolist()\n",
    "                    help_prob = list(chain(*help_prob))\n",
    "                    dict_h = dict(zip(H_nugget, help_prob))\n",
    "                    result.append(dict_h)\n",
    "            # if Id isn't match than continue until it match or switch to new Id\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Submission form\n",
    "        dict1 = {'nugget':result,'id':Id}\n",
    "        final.append(dict1)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3756f6a81ed74adfa9f0516f52118963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final = Generate_submission(dev, lstm_c_model, lstm_h_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate the submission estimation JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "path = 'C:/Users/doudi/OneDrive/Documents/ntcir15/eval'\n",
    "os.chdir(path)\n",
    "timestr = time.strftime(\"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open((timestr + '_' + 'dev_eval.json'), 'w', encoding='utf-8') as f: \n",
    "    f.write(json.dumps(final, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quality': None, 'nugget': {'jsd': 4.565254239683672, 'rnss': 2.9332561476955665}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\eval')\n",
    "!python eval.py 202007081237_dev_eval.json dev_cn.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04223976915092313"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-4.565254239683672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13091876945124023"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-2.9332561476955665)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
