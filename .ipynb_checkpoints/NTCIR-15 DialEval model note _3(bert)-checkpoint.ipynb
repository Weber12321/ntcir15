{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialogue Evaluation\n",
    "###### Created by Weber Huang "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\train_baidu_cn.csv', encoding='utf_8')\n",
    "dev = pd.read_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1\\\\dev_baidu_cn.csv', encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['index'] = train.index\n",
    "dev['index'] = dev.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "\n",
    "def normalize(pred, truth):\n",
    "    \"\"\" convert inputs to np.array and make sure\n",
    "    inputs are normalized probability distributions\n",
    "    \"\"\"\n",
    "    if len(pred) != len(truth):\n",
    "        raise ValueError(\"pred and truth have different lengths\")\n",
    "    if len(pred) == 0 or len(truth) == 0:\n",
    "        raise ValueError(\"pred or truth are empty\")\n",
    "\n",
    "    pred, truth = np.asarray(pred), np.asarray(truth)\n",
    "    if not ((pred >= 0).all() and (truth >= 0).all()):\n",
    "        raise ValueError(\"probability distribution should not be negative\")\n",
    "    pred, truth = pred / pred.sum(), truth / truth.sum()\n",
    "    return pred, truth\n",
    "\n",
    "def jensen_shannon_div(pred, truth, base=2):\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    m = 1. / 2 * (pred + truth)\n",
    "    return (stats.entropy(pred, m, base=base)\n",
    "            + stats.entropy(truth, m, base=base)) / 2.\n",
    "\n",
    "def root_normalized_squared_error(pred, truth):\n",
    "    \"\"\" RNSS: Root Normalised Sum of Squares\n",
    "    \"\"\"\n",
    "\n",
    "    def squared_error(pred, truth):\n",
    "        return ((pred - truth) ** 2).sum()\n",
    "\n",
    "    pred, truth = normalize(pred, truth)\n",
    "    return np.sqrt(squared_error(pred, truth) / 2)\n",
    "\n",
    "def jsd_custom_loss(y_true, y_pred):\n",
    "            \n",
    "    # calculate loss, using y_pred\n",
    "    ''' JSD: Jensen-Shannon Divergence\n",
    "    '''\n",
    "#     y_pred, y_true = normalize(y_pred, y_true)\n",
    "    m = 1. / 2 * (y_pred + y_true)\n",
    "    # loss = (stats.entropy(y_pred, m, base=2) + stats.entropy(y_true, m, base=2)) / 2.\n",
    "    # tf.keras.losses.KLD()\n",
    "    loss = (tf.keras.losses.KLD(y_pred, m) + tf.keras.losses.KLD(y_true, m)) / 2.\n",
    "    return loss\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from keras import Input, Model, losses\n",
    "from keras.layers import Lambda, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "maxlen = 100\n",
    "\n",
    "config_path = 'C:/Users/doudi/Downloads/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = 'C:/Users/doudi/Downloads/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = 'C:/Users/doudi/Downloads/chinese_L-12_H-768_A-12/chinese_L-12_H-768_A-12/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(dict_path):\n",
    "    token_dict = {}\n",
    "    with codecs.open(dict_path, 'r', 'utf-8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    '''\n",
    "    關鍵在  Tokenizer 這個類，要實現這個類中的方法，其實不實現也是可以的\n",
    "    目的是 擴充 vocab.txt文件的\n",
    "    '''\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data():\n",
    "#     data = []\n",
    "#     label = []     \n",
    "#     with codecs.open('C:/Users/doudi/Downloads/BERT/neg.txt', 'r', 'utf-8') as reader:\n",
    "#         for line in reader:\n",
    "#             data.append(line.strip())\n",
    "#             label.append(0)\n",
    "    \n",
    "#     with codecs.open('C:/Users/doudi/Downloads/BERT/pos.txt', 'r', 'utf-8') as reader:\n",
    "#         for line in reader:\n",
    "#             data.append(line.strip())\n",
    "#             label.append(1)    \n",
    "            \n",
    "#     return data, label\n",
    "\n",
    "# train_data = train['utterance'].tolist()\n",
    "# dev_data = dev['utterance'].tolist()\n",
    "\n",
    "# train_label = train[['CNUG0', 'CNUG', 'CNUG*', 'CNaN', 'HNUG', 'HNUG*', 'HNaN']].values.tolist()\n",
    "# dev_label = dev[['CNUG0', 'CNUG', 'CNUG*', 'CNaN', 'HNUG', 'HNUG*', 'HNaN']].values.tolist()\n",
    "# def fill_up_blank(text_list):\n",
    "#     for i in range(len(text_list)):\n",
    "#     if text_list[i]==[]:\n",
    "#         text_list[i] = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(data, token_dict):\n",
    "    '''\n",
    "    :param pos:第一類文本數據\n",
    "    :param neg:第二類文本數據\n",
    "    :param token_dict:編碼字典\n",
    "    :return:[X1,X2]，其中X1是經過編碼後的集合，X2表示第一句和第二句的位置，記錄的是位置信息\n",
    "    '''\n",
    "    tokenizer = OurTokenizer(token_dict)\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    for line in data:\n",
    "        x1, x2 = tokenizer.encode(first=line)\n",
    "        X1.append(x1)\n",
    "        X2.append(x2)\n",
    "    X1 = sequence.pad_sequences(X1, maxlen=maxlen, padding='post', truncating='post')\n",
    "    X2 = sequence.pad_sequences(X2, maxlen=maxlen, padding='post', truncating='post')\n",
    "    return [X1,X2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bert_model_c(X1, X2, y):\n",
    "    '''\n",
    "    :param X1:經過編碼過後的集合\n",
    "    :param X2:經過編碼過後的位置集合\n",
    "    :return:模型\n",
    "    '''\n",
    "    bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "    x1 = Input(shape=(None,))\n",
    "    x2 = Input(shape=(None,))\n",
    "    x = bert_model([x1, x2])\n",
    "    x = Lambda(lambda x: x[:, 0])(x)\n",
    "#     y = np_utils.to_categorical(y)\n",
    "    p = Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = Model([x1,x2],p)\n",
    "    model.compile(loss=jsd_custom_loss, optimizer=Adam(1e-3), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit([X1, X2], y, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_bert_model_h(X1, X2, y):\n",
    "    '''\n",
    "    :param X1:經過編碼過後的集合\n",
    "    :param X2:經過編碼過後的位置集合\n",
    "    :return:模型\n",
    "    '''\n",
    "    bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "    x1 = Input(shape=(None,))\n",
    "    x2 = Input(shape=(None,))\n",
    "    x = bert_model([x1, x2])\n",
    "    x = Lambda(lambda x: x[:, 0])(x)\n",
    "#     y = np_utils.to_categorical(y)\n",
    "    p = Dense(3, activation='softmax')(x)\n",
    "\n",
    "    model = Model([x1,x2],p)\n",
    "    model.compile(loss=jsd_custom_loss, optimizer=Adam(1e-3), metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    model.fit([X1, X2], y, epochs=10, batch_size=128, validation_split=0.2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split from sender\n",
    "# cus\n",
    "train_c = train[train.sender=='customer']['texts']\n",
    "train_c_label = train[train.sender=='customer'][['CNUG0', 'CNUG', 'CNUG*', 'CNaN']].values.tolist()\n",
    "\n",
    "dev_c = dev[dev.sender=='customer']['texts']\n",
    "dev_c_label = dev[dev.sender=='customer'][['CNUG0', 'CNUG', 'CNUG*', 'CNaN']].values.tolist()\n",
    "\n",
    "# help\n",
    "train_h = train[train.sender=='helpdesk']['texts']\n",
    "train_h_label = train[train.sender=='helpdesk'][['HNUG', 'HNUG*', 'HNaN']].values.tolist()\n",
    "\n",
    "dev_h = dev[dev.sender=='helpdesk']['texts']\n",
    "dev_h_label = dev[dev.sender=='helpdesk'][['HNUG', 'HNUG*', 'HNaN']].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# data, label = get_data()\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.33, random_state=42)\n",
    "token_dict = get_token_dict(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, None, 768)    101677056   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            3076        lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 101,680,132\n",
      "Trainable params: 3,076\n",
      "Non-trainable params: 101,677,056\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/10\n",
      "6800/6800 [==============================] - 81s 12ms/step - loss: 0.1240 - accuracy: 0.6254 - val_loss: 0.0861 - val_accuracy: 0.7547\n",
      "Epoch 2/10\n",
      "6800/6800 [==============================] - 78s 11ms/step - loss: 0.0909 - accuracy: 0.7162 - val_loss: 0.0746 - val_accuracy: 0.7600\n",
      "Epoch 3/10\n",
      "6800/6800 [==============================] - 78s 11ms/step - loss: 0.0829 - accuracy: 0.7312 - val_loss: 0.0692 - val_accuracy: 0.7894\n",
      "Epoch 4/10\n",
      "6800/6800 [==============================] - 77s 11ms/step - loss: 0.0784 - accuracy: 0.7469 - val_loss: 0.0673 - val_accuracy: 0.7876\n",
      "Epoch 5/10\n",
      "6800/6800 [==============================] - 75s 11ms/step - loss: 0.0766 - accuracy: 0.7503 - val_loss: 0.0659 - val_accuracy: 0.7888\n",
      "Epoch 6/10\n",
      "2304/6800 [=========>....................] - ETA: 41s - loss: 0.0739 - accuracy: 0.7574"
     ]
    }
   ],
   "source": [
    "X1_train, X2_train = get_encode(train_c, token_dict)\n",
    "Cus = build_bert_model_c(X1_train, X2_train, np.array(train_c_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X2_train = get_encode(train_h, token_dict)\n",
    "Hel = build_bert_model_h(X1_train, X2_train, np.array(train_h_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_c = pd.DataFrame(list(zip(dev[dev.sender=='customer']['index'].tolist(), \n",
    "                                 Cus.predict(get_encode(dev_c, token_dict)).tolist())), \n",
    "                        columns =['Index', 'label'])\n",
    "output_h = pd.DataFrame(list(zip(dev[dev.sender=='helpdesk']['index'].tolist(), \n",
    "                                 Hel.predict(get_encode(dev_h, token_dict)).tolist())), \n",
    "                        columns =['Index', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>sender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.6461959481239319, 0.14843349158763885, 0.02...</td>\n",
       "      <td>4227729258237823</td>\n",
       "      <td>customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.38930705189704895, 0.07891961187124252, 0.5...</td>\n",
       "      <td>4227729258237823</td>\n",
       "      <td>helpdesk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.08148171007633209, 0.7746328711509705, 0.00...</td>\n",
       "      <td>4227729258237823</td>\n",
       "      <td>customer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.49477341771125793, 0.05395147576928139, 0.4...</td>\n",
       "      <td>4227729258237823</td>\n",
       "      <td>helpdesk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.4355168342590332, 0.30214032530784607, 0.01...</td>\n",
       "      <td>4121001149457182</td>\n",
       "      <td>customer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                              label                id  \\\n",
       "0      0  [0.6461959481239319, 0.14843349158763885, 0.02...  4227729258237823   \n",
       "1      1  [0.38930705189704895, 0.07891961187124252, 0.5...  4227729258237823   \n",
       "2      2  [0.08148171007633209, 0.7746328711509705, 0.00...  4227729258237823   \n",
       "3      3  [0.49477341771125793, 0.05395147576928139, 0.4...  4227729258237823   \n",
       "4      4  [0.4355168342590332, 0.30214032530784607, 0.01...  4121001149457182   \n",
       "\n",
       "     sender  \n",
       "0  customer  \n",
       "1  helpdesk  \n",
       "2  customer  \n",
       "3  helpdesk  \n",
       "4  customer  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.concat([output_c,output_h], ignore_index=True)  \n",
    "output = output.sort_values(by=['Index'])\n",
    "output = output.reset_index(drop=True)\n",
    "output['id'] = dev['id'].apply(str)\n",
    "output['sender'] = dev['sender']\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'customer'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_submission(dev):\n",
    "    dev['id'] = dev['id'].apply(str)\n",
    "    Id_list = dev['id'].unique()\n",
    "    C_nugget = ['CNUG0','CNUG','CNUG*','CNaN']\n",
    "    H_nugget = ['HNUG','HNUG*','HNaN']\n",
    "\n",
    "    final = []\n",
    "    \n",
    "    # go through each Id first\n",
    "    for Id in tqdm(Id_list):  \n",
    "        result = []\n",
    "        \n",
    "        for i in range(len(dev)):\n",
    "            # if Id is match than predict the prob_distribution and zip it as dictionary \n",
    "            if dev['id'][i] == Id:\n",
    "                if dev.iloc[i, 3] == 'customer':\n",
    "                    cus_prob = dev['label'][i]\n",
    "                    dict_c = dict(zip(C_nugget, cus_prob))\n",
    "                    result.append(dict_c)\n",
    "                else:\n",
    "                    help_prob = dev['label'][i]\n",
    "                    dict_h = dict(zip(H_nugget, help_prob))\n",
    "                    result.append(dict_h)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Submission form\n",
    "        dict1 = {'nugget':result,'id':Id}\n",
    "        final.append(dict1)\n",
    "        \n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770bf4ec8a074b8088daeb82a5a3496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final = Generate_submission(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "path = 'C:/Users/doudi/OneDrive/Documents/ntcir15/eval'\n",
    "os.chdir(path)\n",
    "timestr = time.strftime(\"%Y%m%d%H%M\")\n",
    "with open((timestr + '_' + 'dev_eval.json'), 'w', encoding='utf-8') as f: \n",
    "    f.write(json.dumps(final, ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quality': None, 'nugget': {'jsd': 3.1553373609362283, 'rnss': 2.091356544537677}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\eval')\n",
    "!python eval.py 202007141514_dev_eval.json dev_cn.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11224029718267788"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-3.1553373609362283)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import concatenate, Bidirectional\n",
    "from keras import initializers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Features preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n"
     ]
    }
   ],
   "source": [
    "# === customer\n",
    "c_X_train = train_c.filter(['round','texts'])\n",
    "c_X_test = dev_c.filter(['round','texts'])\n",
    "    \n",
    "\n",
    "y_train_c = train_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "y_test_c = dev_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "\n",
    "# y_train_h = train.filter(['HNUG','HNUG*','HNaN'])\n",
    "# y_test_h = dev.filter(['HNUG','HNUG*','HNaN'])\n",
    "\n",
    "c_X1_train = c_X_train['texts']\n",
    "# c_X1_train = [str (item) for item in c_X1_train]\n",
    "c_X1_test = c_X_test['texts']\n",
    "# c_X1_test = [str (item) for item in c_X1_test]\n",
    "\n",
    "c_X2_train = c_X_train[['round']].values\n",
    "c_X2_test = c_X_test[['round']].values\n",
    "\n",
    "c_token = Tokenizer(num_words = 20000)\n",
    "c_token.fit_on_texts(c_X1_train)\n",
    "c_vocab = c_token.word_index\n",
    "print(c_token.document_count)\n",
    "\n",
    "c_x_train_seq = c_token.texts_to_sequences(c_X1_train)\n",
    "c_x_test_seq = c_token.texts_to_sequences(c_X1_test)\n",
    "c_X1_train = sequence.pad_sequences(c_x_train_seq, maxlen = 150)\n",
    "c_X1_test = sequence.pad_sequences(c_x_test_seq, maxlen = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6900\n"
     ]
    }
   ],
   "source": [
    "# === helpdesk\n",
    "h_X_train = train_h.filter(['round','texts'])\n",
    "h_X_test = dev_h.filter(['round','texts'])\n",
    "    \n",
    "\n",
    "# y_train_c = train_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "# y_test_c = dev_c.filter(['CNUG','CNUG*','CNUG0','CNaN'])\n",
    "\n",
    "y_train_h = train_h.filter(['HNUG','HNUG*','HNaN'])\n",
    "y_test_h = dev_h.filter(['HNUG','HNUG*','HNaN'])\n",
    "\n",
    "h_X1_train = h_X_train['texts']\n",
    "# h_X1_train = [str (item) for item in h_X1_train]\n",
    "h_X1_test = h_X_test['texts']\n",
    "# h_X1_test = [str (item) for item in h_X1_test]\n",
    "\n",
    "h_X2_train = h_X_train[['round']].values\n",
    "h_X2_test = h_X_test[['round']].values\n",
    "\n",
    "h_token = Tokenizer(num_words = 20000)\n",
    "h_token.fit_on_texts(h_X1_train)\n",
    "h_vocab = h_token.word_index\n",
    "print(h_token.document_count)\n",
    "\n",
    "h_x_train_seq = h_token.texts_to_sequences(h_X1_train)\n",
    "h_x_test_seq = h_token.texts_to_sequences(h_X1_test)\n",
    "h_X1_train = sequence.pad_sequences(h_x_train_seq, maxlen = 150)\n",
    "h_X1_test = sequence.pad_sequences(h_x_test_seq, maxlen = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 textCNN for customer and helpdesk respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === customer\n",
    "# def CNN_C(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):   \n",
    "    \n",
    "#     num_labels = 4\n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "\n",
    "#     sub_input = Input(shape=(2,))\n",
    "    \n",
    "#     # pre-train embeddings\n",
    "#     # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "#     # embed = embedder(main_input)\n",
    "#     embed = Embedding(len(c_vocab)+1, 300, input_length=150)(main_input)\n",
    "#     # filter size, region size\n",
    "#     cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "#     cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "#     flat = Flatten()(cnn)\n",
    "#     drop = Dropout(0.2)(flat)\n",
    "#     # main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "\n",
    "\n",
    "#     dense_1 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     drop_1 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(drop_1)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([drop, dense_2])\n",
    "#     dense_3 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=4, activation='softmax')(dense_3)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=10, \n",
    "#                               batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === helpdesk\n",
    "# def CNN_H(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "    \n",
    "#     num_labels = 2\n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "\n",
    "#     sub_input = Input(shape=(2,))\n",
    "    \n",
    "#     # pre-train embeddings\n",
    "#     # embedder = Embedding(len(vocab) + 1, 300, input_length = 20, weights = [embedding_matrix], trainable = False)\n",
    "#     # embed = embedder(main_input)\n",
    "#     embed = Embedding(len(h_vocab)+1, 300, input_length=150)(main_input)\n",
    "#     # filter size, region size\n",
    "#     cnn = Convolution1D(2, 2, padding='same', strides = 1, activation='relu')(embed)\n",
    "#     cnn = MaxPool1D(pool_size=4)(cnn)\n",
    "#     flat = Flatten()(cnn)\n",
    "#     drop = Dropout(0.2)(flat)\n",
    "#     # main_output = Dense(num_labels, activation='sigmoid')(drop)\n",
    "\n",
    "\n",
    "#     dense_1 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     drop_1 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(drop_1)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([drop, dense_2])\n",
    "#     dense_3 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=3, activation='softmax')(dense_3)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=10, \n",
    "#                               batch_size=64, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 LSTM for customer and helpdesk respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === customer\n",
    "# def lstm_C(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "     \n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "#     sub_input = Input(shape=(1,))\n",
    "    \n",
    "#     embed = Embedding(output_dim=32,input_dim=20000,input_length=150)(main_input)\n",
    "#     dropout_1 = Dropout(0.35)(embed)\n",
    "#     lst = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_1)\n",
    "#     lst2 = Bidirectional(LSTM(units=16))(lst)\n",
    "#     dense_1 = Dense(units=256,activation='relu')(lst2)\n",
    "#     dropout_2 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(dropout_2)\n",
    "#     dense_3 = Dense(units=4,activation='softmax')(dense_2)\n",
    "\n",
    "\n",
    "#     dense_4 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     dropout_3 = Dropout(0.35)(dense_4)\n",
    "# #     lst2 = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_3)\n",
    "#     dense_5 = Dense(units=128,activation='relu')(dropout_3)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([dense_3, dense_5])\n",
    "#     dense_6 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=4, activation='softmax')(dense_6)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "#                               batch_size=128, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === helpdesk\n",
    "# def lstm_H(X1_train, X2_train, X1_test, X2_test, y_train, y_test, loss='categorical_crossentropy'):\n",
    "     \n",
    "#     main_input = Input(shape=(150,), dtype='float64')\n",
    "#     sub_input = Input(shape=(1,))\n",
    "    \n",
    "#     embed = Embedding(output_dim=32,input_dim=20000,input_length=150)(main_input)\n",
    "#     dropout_1 = Dropout(0.35)(embed)\n",
    "#     lst = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_1)\n",
    "#     lst2 = Bidirectional(LSTM(units=16))(lst)\n",
    "#     dense_1 = Dense(units=256,activation='relu')(lst2)\n",
    "#     dropout_2 = Dropout(0.35)(dense_1)\n",
    "#     dense_2 = Dense(units=128,activation='relu')(dropout_2)\n",
    "#     dense_3 = Dense(units=3,activation='softmax')(dense_2)\n",
    "\n",
    "\n",
    "#     dense_4 = Dense(units=256,activation='relu')(sub_input)\n",
    "#     dropout_3 = Dropout(0.35)(dense_4)\n",
    "# #     lst2 = Bidirectional(LSTM(units=16,return_sequences=True))(dropout_3)\n",
    "#     dense_5 = Dense(units=128,activation='relu')(dropout_3)\n",
    "#     # sub_output = Dense(units=2,activation='sigmoid')(dense_2)\n",
    "\n",
    "#     merge = concatenate([dense_3, dense_5])\n",
    "#     dense_6 = Dense(units=10, activation='relu')(merge)\n",
    "#     output = Dense(units=3, activation='softmax')(dense_6)\n",
    "\n",
    "#     model = Model(inputs=[main_input, sub_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "\n",
    "#     # checkpoint\n",
    "#     # filepath=\"C:/Users/doudi/OneDrive/Documents/TMU-GIDS/Lab/Competition/AI cup 2019/weights.best.hdf5\"\n",
    "#     # checkpoint= ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "#     train_history = model.fit(x=[X1_train, X2_train], y=y_train, epochs=50, \n",
    "#                               batch_size=128, verbose=2, validation_split=0.2)\n",
    "\n",
    "#     score = model.evaluate(x=[X1_test, X2_test], y=y_test, verbose=1)\n",
    "\n",
    "#     print(\"Test Score:\", score[0])\n",
    "#     print(\"Test Accuracy:\", score[1])\n",
    "\n",
    "#     pre_probability = model.predict(x=[X1_test, X2_test])\n",
    "#     predicted = pre_probability.argmax(axis=-1)\n",
    "    \n",
    "#     return model, train_history, pre_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Subsetting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN_c_model, CNN_c_history, CNN_c_pred = CNN_C(c_X1_train, c_X2_train, c_X1_test, \n",
    "#                                                c_X2_test, y_train_c, y_test_c, \n",
    "#                                                loss = jsd_custom_loss)\n",
    "\n",
    "# CNN_h_model, CNN_h_history, CNN_h_pred = CNN_H(h_X1_train, h_X2_train, h_X1_test, \n",
    "#                                                h_X2_test, y_train_h, y_test_h, \n",
    "#                                                loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 150, 32)      640000      input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 150, 32)      0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 150, 32)      6272        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 32)           6272        bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 256)          8448        bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 256)          0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 256)          512         input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 128)          32896       dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 256)          0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 4)            516         dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 128)          32896       dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 132)          0           dense_29[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 10)           1330        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 4)            44          dense_32[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 729,186\n",
      "Trainable params: 729,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 6800 samples, validate on 1700 samples\n",
      "Epoch 1/50\n",
      " - 21s - loss: 0.1549 - accuracy: 0.5263 - val_loss: 0.1090 - val_accuracy: 0.8747\n",
      "Epoch 2/50\n",
      " - 20s - loss: 0.0727 - accuracy: 0.8510 - val_loss: 0.0492 - val_accuracy: 0.8747\n",
      "Epoch 3/50\n",
      " - 19s - loss: 0.0509 - accuracy: 0.8525 - val_loss: 0.0441 - val_accuracy: 0.8747\n",
      "Epoch 4/50\n",
      " - 19s - loss: 0.0455 - accuracy: 0.8518 - val_loss: 0.0413 - val_accuracy: 0.8747\n",
      "Epoch 5/50\n",
      " - 20s - loss: 0.0422 - accuracy: 0.8512 - val_loss: 0.0408 - val_accuracy: 0.8747\n",
      "Epoch 6/50\n",
      " - 19s - loss: 0.0402 - accuracy: 0.8507 - val_loss: 0.0426 - val_accuracy: 0.8635\n",
      "Epoch 7/50\n",
      " - 19s - loss: 0.0388 - accuracy: 0.8546 - val_loss: 0.0408 - val_accuracy: 0.8747\n",
      "Epoch 8/50\n",
      " - 19s - loss: 0.0380 - accuracy: 0.8541 - val_loss: 0.0409 - val_accuracy: 0.8700\n",
      "Epoch 9/50\n",
      " - 19s - loss: 0.0372 - accuracy: 0.8521 - val_loss: 0.0427 - val_accuracy: 0.8459\n",
      "Epoch 10/50\n",
      " - 19s - loss: 0.0362 - accuracy: 0.8559 - val_loss: 0.0408 - val_accuracy: 0.8771\n",
      "Epoch 11/50\n",
      " - 19s - loss: 0.0359 - accuracy: 0.8574 - val_loss: 0.0414 - val_accuracy: 0.8535\n",
      "Epoch 12/50\n",
      " - 19s - loss: 0.0351 - accuracy: 0.8616 - val_loss: 0.0415 - val_accuracy: 0.8712\n",
      "Epoch 13/50\n",
      " - 19s - loss: 0.0347 - accuracy: 0.8632 - val_loss: 0.0417 - val_accuracy: 0.8659\n",
      "Epoch 14/50\n",
      " - 19s - loss: 0.0342 - accuracy: 0.8619 - val_loss: 0.0434 - val_accuracy: 0.8365\n",
      "Epoch 15/50\n",
      " - 19s - loss: 0.0336 - accuracy: 0.8647 - val_loss: 0.0418 - val_accuracy: 0.8665\n",
      "Epoch 16/50\n",
      " - 19s - loss: 0.0332 - accuracy: 0.8647 - val_loss: 0.0416 - val_accuracy: 0.8629\n",
      "Epoch 17/50\n",
      " - 19s - loss: 0.0327 - accuracy: 0.8704 - val_loss: 0.0422 - val_accuracy: 0.8612\n",
      "Epoch 18/50\n",
      " - 19s - loss: 0.0323 - accuracy: 0.8776 - val_loss: 0.0420 - val_accuracy: 0.8724\n",
      "Epoch 19/50\n",
      " - 19s - loss: 0.0322 - accuracy: 0.8784 - val_loss: 0.0423 - val_accuracy: 0.8671\n",
      "Epoch 20/50\n",
      " - 19s - loss: 0.0315 - accuracy: 0.8865 - val_loss: 0.0421 - val_accuracy: 0.8641\n",
      "Epoch 21/50\n",
      " - 19s - loss: 0.0312 - accuracy: 0.8940 - val_loss: 0.0423 - val_accuracy: 0.8653\n",
      "Epoch 22/50\n",
      " - 19s - loss: 0.0312 - accuracy: 0.8918 - val_loss: 0.0435 - val_accuracy: 0.8465\n",
      "Epoch 23/50\n",
      " - 19s - loss: 0.0308 - accuracy: 0.8953 - val_loss: 0.0422 - val_accuracy: 0.8618\n",
      "Epoch 24/50\n",
      " - 19s - loss: 0.0302 - accuracy: 0.8966 - val_loss: 0.0417 - val_accuracy: 0.8676\n",
      "Epoch 25/50\n",
      " - 19s - loss: 0.0299 - accuracy: 0.9015 - val_loss: 0.0418 - val_accuracy: 0.8700\n",
      "Epoch 26/50\n",
      " - 19s - loss: 0.0297 - accuracy: 0.9000 - val_loss: 0.0419 - val_accuracy: 0.8724\n",
      "Epoch 27/50\n",
      " - 19s - loss: 0.0295 - accuracy: 0.9004 - val_loss: 0.0422 - val_accuracy: 0.8712\n",
      "Epoch 28/50\n",
      " - 19s - loss: 0.0293 - accuracy: 0.9018 - val_loss: 0.0428 - val_accuracy: 0.8588\n",
      "Epoch 29/50\n",
      " - 19s - loss: 0.0291 - accuracy: 0.8996 - val_loss: 0.0422 - val_accuracy: 0.8665\n",
      "Epoch 30/50\n",
      " - 19s - loss: 0.0289 - accuracy: 0.9038 - val_loss: 0.0426 - val_accuracy: 0.8682\n",
      "Epoch 31/50\n",
      " - 19s - loss: 0.0290 - accuracy: 0.9024 - val_loss: 0.0423 - val_accuracy: 0.8741\n",
      "Epoch 32/50\n",
      " - 19s - loss: 0.0289 - accuracy: 0.9044 - val_loss: 0.0434 - val_accuracy: 0.8465\n",
      "Epoch 33/50\n",
      " - 19s - loss: 0.0287 - accuracy: 0.9016 - val_loss: 0.0420 - val_accuracy: 0.8671\n",
      "Epoch 34/50\n",
      " - 19s - loss: 0.0284 - accuracy: 0.9074 - val_loss: 0.0427 - val_accuracy: 0.8582\n",
      "Epoch 35/50\n",
      " - 19s - loss: 0.0283 - accuracy: 0.9015 - val_loss: 0.0418 - val_accuracy: 0.8700\n",
      "Epoch 36/50\n",
      " - 19s - loss: 0.0284 - accuracy: 0.9059 - val_loss: 0.0422 - val_accuracy: 0.8641\n",
      "Epoch 37/50\n",
      " - 19s - loss: 0.0281 - accuracy: 0.9063 - val_loss: 0.0424 - val_accuracy: 0.8635\n",
      "Epoch 38/50\n",
      " - 19s - loss: 0.0280 - accuracy: 0.9090 - val_loss: 0.0426 - val_accuracy: 0.8571\n",
      "Epoch 39/50\n",
      " - 19s - loss: 0.0279 - accuracy: 0.9079 - val_loss: 0.0423 - val_accuracy: 0.8635\n",
      "Epoch 40/50\n",
      " - 19s - loss: 0.0278 - accuracy: 0.9075 - val_loss: 0.0425 - val_accuracy: 0.8665\n",
      "Epoch 41/50\n",
      " - 19s - loss: 0.0277 - accuracy: 0.9062 - val_loss: 0.0419 - val_accuracy: 0.8671\n",
      "Epoch 42/50\n",
      " - 19s - loss: 0.0276 - accuracy: 0.9097 - val_loss: 0.0424 - val_accuracy: 0.8665\n",
      "Epoch 43/50\n",
      " - 19s - loss: 0.0277 - accuracy: 0.9065 - val_loss: 0.0420 - val_accuracy: 0.8676\n",
      "Epoch 44/50\n",
      " - 19s - loss: 0.0275 - accuracy: 0.9079 - val_loss: 0.0417 - val_accuracy: 0.8694\n",
      "Epoch 45/50\n",
      " - 19s - loss: 0.0274 - accuracy: 0.9118 - val_loss: 0.0420 - val_accuracy: 0.8612\n",
      "Epoch 46/50\n",
      " - 19s - loss: 0.0273 - accuracy: 0.9091 - val_loss: 0.0421 - val_accuracy: 0.8635\n",
      "Epoch 47/50\n",
      " - 19s - loss: 0.0272 - accuracy: 0.9085 - val_loss: 0.0416 - val_accuracy: 0.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      " - 19s - loss: 0.0270 - accuracy: 0.9119 - val_loss: 0.0419 - val_accuracy: 0.8700\n",
      "Epoch 49/50\n",
      " - 19s - loss: 0.0272 - accuracy: 0.9084 - val_loss: 0.0426 - val_accuracy: 0.8488\n",
      "Epoch 50/50\n",
      " - 19s - loss: 0.0269 - accuracy: 0.9119 - val_loss: 0.0415 - val_accuracy: 0.8612\n",
      "975/975 [==============================] - 4s 4ms/step\n",
      "Test Score: 0.0402319198464736\n",
      "Test Accuracy: 0.8482051491737366\n"
     ]
    }
   ],
   "source": [
    "# lstm_c_model, lstm_c_history, lstm_c_pred = lstm_C(c_X1_train, c_X2_train, c_X1_test, \n",
    "#                                                c_X2_test, y_train_c, y_test_c, \n",
    "#                                                loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 150, 32)      640000      input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 150, 32)      0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 150, 32)      6272        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 32)           6272        bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 256)          8448        bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 256)          0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 256)          512         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 128)          32896       dropout_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 256)          0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 3)            387         dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 128)          32896       dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 131)          0           dense_36[0][0]                   \n",
      "                                                                 dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 10)           1320        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 3)            33          dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 729,036\n",
      "Trainable params: 729,036\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 5520 samples, validate on 1380 samples\n",
      "Epoch 1/50\n",
      " - 17s - loss: 0.0604 - accuracy: 0.7560 - val_loss: 0.0537 - val_accuracy: 0.8217\n",
      "Epoch 2/50\n",
      " - 16s - loss: 0.0524 - accuracy: 0.8147 - val_loss: 0.0514 - val_accuracy: 0.8217\n",
      "Epoch 3/50\n",
      " - 16s - loss: 0.0494 - accuracy: 0.8147 - val_loss: 0.0485 - val_accuracy: 0.8217\n",
      "Epoch 4/50\n",
      " - 16s - loss: 0.0469 - accuracy: 0.8147 - val_loss: 0.0468 - val_accuracy: 0.8217\n",
      "Epoch 5/50\n",
      " - 16s - loss: 0.0461 - accuracy: 0.8147 - val_loss: 0.0469 - val_accuracy: 0.8217\n",
      "Epoch 6/50\n",
      " - 15s - loss: 0.0460 - accuracy: 0.8147 - val_loss: 0.0466 - val_accuracy: 0.8217\n",
      "Epoch 7/50\n",
      " - 16s - loss: 0.0458 - accuracy: 0.8147 - val_loss: 0.0465 - val_accuracy: 0.8217\n",
      "Epoch 8/50\n",
      " - 16s - loss: 0.0456 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 9/50\n",
      " - 16s - loss: 0.0457 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 10/50\n",
      " - 16s - loss: 0.0456 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 11/50\n",
      " - 15s - loss: 0.0456 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 12/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0464 - val_accuracy: 0.8217\n",
      "Epoch 13/50\n",
      " - 15s - loss: 0.0456 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 14/50\n",
      " - 15s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 15/50\n",
      " - 16s - loss: 0.0456 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 16/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0465 - val_accuracy: 0.8217\n",
      "Epoch 17/50\n",
      " - 15s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0464 - val_accuracy: 0.8217\n",
      "Epoch 18/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 19/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 20/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 21/50\n",
      " - 16s - loss: 0.0456 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 22/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0464 - val_accuracy: 0.8217\n",
      "Epoch 23/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 24/50\n",
      " - 15s - loss: 0.0456 - accuracy: 0.8147 - val_loss: 0.0464 - val_accuracy: 0.8217\n",
      "Epoch 25/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 26/50\n",
      " - 15s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 27/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 28/50\n",
      " - 15s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0465 - val_accuracy: 0.8217\n",
      "Epoch 29/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 30/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 31/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 32/50\n",
      " - 15s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 33/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 34/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 35/50\n",
      " - 16s - loss: 0.0455 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 36/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0463 - val_accuracy: 0.8217\n",
      "Epoch 37/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 38/50\n",
      " - 16s - loss: 0.0453 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 39/50\n",
      " - 16s - loss: 0.0453 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 40/50\n",
      " - 16s - loss: 0.0453 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 41/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "Epoch 42/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 43/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 44/50\n",
      " - 16s - loss: 0.0453 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 45/50\n",
      " - 16s - loss: 0.0453 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 46/50\n",
      " - 15s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 47/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 49/50\n",
      " - 16s - loss: 0.0454 - accuracy: 0.8147 - val_loss: 0.0461 - val_accuracy: 0.8217\n",
      "Epoch 50/50\n",
      " - 16s - loss: 0.0453 - accuracy: 0.8147 - val_loss: 0.0462 - val_accuracy: 0.8217\n",
      "780/780 [==============================] - 3s 4ms/step\n",
      "Test Score: 0.04450143308211595\n",
      "Test Accuracy: 0.8384615182876587\n"
     ]
    }
   ],
   "source": [
    "# lstm_h_model, lstm_h_history, lstm_h_pred = lstm_H(h_X1_train, h_X2_train, h_X1_test, \n",
    "#                                                h_X2_test, y_train_h, y_test_h,  \n",
    "#                                                loss = jsd_custom_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Output by rbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict single row\n",
    "def padding_single_c(dev):\n",
    "    X_test = dev.filter(['round','texts'])\n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = [str (item) for item in X1_test]\n",
    "    X2_test = X_test[['round']].values\n",
    "    \n",
    "    x_test_seq = c_token.texts_to_sequences([X1_test])\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 150)\n",
    "    \n",
    "    return X1_test, X2_test\n",
    "\n",
    "def padding_single_h(dev):\n",
    "    X_test = dev.filter(['round','texts'])\n",
    "    X1_test = X_test['texts']\n",
    "#     X1_test = [str (item) for item in X1_test]\n",
    "    X2_test = X_test[['round']].values\n",
    "    \n",
    "    x_test_seq = h_token.texts_to_sequences([X1_test])\n",
    "    X1_test = sequence.pad_sequences(x_test_seq, maxlen = 150)\n",
    "    \n",
    "    return X1_test, X2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# input the development dataframe and the method\n",
    "# for current models (loss = jsd):\n",
    "# model_1 = customer model (CNN_c_model, lstm_c_model)\n",
    "# model_2 = helpdesk model (CNN_h_model, lstm_h_model)\n",
    "def Generate_submission(dev, model_1, model_2):\n",
    "    Id_list = dev['id'].unique()\n",
    "    C_nugget = ['CNUG','CNUG*','CNUG0','CNaN']\n",
    "    H_nugget = ['HNUG','HNUG*','HNaN']\n",
    "\n",
    "    final = []\n",
    "    \n",
    "    # go through each Id first\n",
    "    for Id in tqdm(Id_list):  \n",
    "        result = []\n",
    "        \n",
    "        for i in range(len(dev)):\n",
    "            \n",
    "            # if Id is match than predict the prob_distribution and zip it as dictionary \n",
    "            if dev['id'][i] == Id:\n",
    "                if dev.iloc[i, 1] == 'customer':\n",
    "                    t1, t2 = padding_single_c(dev.iloc[i])\n",
    "                    t2 = np.array(t2).reshape(1,1)\n",
    "                    cus_prob = model_1.predict(x=[t1, t2])\n",
    "                    cus_prob = cus_prob.tolist()\n",
    "                    cus_prob = list(chain(*cus_prob))\n",
    "                    dict_c = dict(zip(C_nugget, cus_prob))\n",
    "                    result.append(dict_c)\n",
    "                else:\n",
    "                    t3, t4 = padding_single_h(dev.iloc[i])\n",
    "                    t4 = np.array(t4).reshape(1,1)\n",
    "                    help_prob = model_2.predict(x=[t3, t4])\n",
    "                    help_prob = help_prob.tolist()\n",
    "                    help_prob = list(chain(*help_prob))\n",
    "                    dict_h = dict(zip(H_nugget, help_prob))\n",
    "                    result.append(dict_h)\n",
    "            # if Id isn't match than continue until it match or switch to new Id\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Submission form\n",
    "        dict1 = {'nugget':result,'id':Id}\n",
    "        final.append(dict1)\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c49d7de2e90467a9665953ea14a8a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=390.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "(1, 150) (1,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = Generate_submission(dev, lstm_c_model, lstm_h_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Generate the submission estimation JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "path = 'C:/Users/doudi/OneDrive/Documents/ntcir15/eval'\n",
    "os.chdir(path)\n",
    "timestr = time.strftime(\"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open((timestr + '_' + 'dev_eval.json'), 'w', encoding='utf-8') as f: \n",
    "    f.write(json.dumps(final, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quality': None, 'nugget': {'jsd': 4.08938488356849, 'rnss': 2.628283285580723}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\eval')\n",
    "!python eval.py 202007071218_dev_eval.json dev_cn.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058745213461662074"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-4.08938488356849)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2**(-2.80168596346675)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
