{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for test data preprocess only!\n",
    "def pre_test(dataframe):    \n",
    "    length = []\n",
    "    for i in tqdm(dataframe['turns']):\n",
    "        length.append(len(i))\n",
    "    Id = dataframe['id'].tolist()\n",
    "\n",
    "    Fin_Id = sum([[s] * n for s, n in zip(Id, length)], [])\n",
    "    turns_list = dataframe['turns'].tolist()\n",
    "    Fin_turns_anno = []\n",
    "    for x in tqdm(turns_list):\n",
    "        for q in range(len(x)):\n",
    "            Fin_turns_anno.append(list(x[q].values()))\n",
    "\n",
    "    train_clean = pd.DataFrame({'id': Fin_Id,'info': Fin_turns_anno})\n",
    "    train_df = pd.DataFrame(train_clean['info'].values.tolist(), columns=['sender','utterance'])\n",
    "    train_df['id'] = train_clean['id']\n",
    "    train_df = train_df[['id','sender','utterance']]\n",
    "\n",
    "    # id to str\n",
    "    train_df['id'] = train_df['id'].apply(str) \n",
    "    # round\n",
    "    uni = train_df.id.unique()\n",
    "    num = []\n",
    "    for i in uni:\n",
    "        count = 1\n",
    "        for j in train_df['id']:\n",
    "            if i == j:\n",
    "                num.append(count)\n",
    "                count += 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    train_df['round'] = num\n",
    "    # label encoding (sender_num)\n",
    "    sender = ['customer','helpdesk']\n",
    "    l = preprocessing.LabelEncoder()\n",
    "    l.fit(sender);\n",
    "    sender_num = l.transform(list(train_df['sender']))\n",
    "    train_df['sender_num'] = sender_num\n",
    "    subset = train_df[['id','sender','sender_num','utterance','round']]\n",
    "    return subset\n",
    "\n",
    "def segment_2(dataframe, file_path):\n",
    "    texts = dataframe['utterance'].astype(str)\n",
    "    seg_texts = []\n",
    "    for line in texts:\n",
    "        seg_content = ' '.join(jieba.cut(line, cut_all = False))\n",
    "        seg_texts.append(seg_content)\n",
    "    def remove_punctuation(line):\n",
    "        rule = re.compile(\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "        line = rule.sub(' ',line)\n",
    "        return line\n",
    "    texts = []\n",
    "    for line in seg_texts:\n",
    "        new_line = remove_punctuation(line).split()\n",
    "        texts.append(new_line)  \n",
    "    cn_stopwords = []\n",
    "    with open(file_path, 'r', encoding='UTF-8') as file:\n",
    "        for data in file.read().splitlines():\n",
    "            cn_stopwords.append(data)\n",
    "    # remove punctuation\n",
    "    pp_texts = []\n",
    "    for line in texts:\n",
    "        line_noSW = []\n",
    "        for word in line:\n",
    "            if word not in cn_stopwords:\n",
    "                line_noSW.append(word)\n",
    "        pp_texts.append(line_noSW)\n",
    "    # change emoji in pp_texts to *\n",
    "    for line in pp_texts:\n",
    "        if line == []:\n",
    "            line.append(\"*\")      \n",
    "    # concatenate the sentences by whitespace\n",
    "    new_texts = []\n",
    "    for sentence in pp_texts:\n",
    "        series_sentence = \" \".join(word for word in sentence)\n",
    "        new_texts.append(series_sentence)\n",
    "    dataframe['texts'] = new_texts\n",
    "    subset = dataframe[['id','sender','sender_num','texts','round',]]\n",
    "    return subset\n",
    "\n",
    "def test_preprocess(name, wd, stop_word_path):\n",
    "    os.chdir(wd)\n",
    "    file = pd.read_json(name, encoding='utf8')\n",
    "    t = pre_test(file)\n",
    "    tmp = segment_2(t, stop_word_path)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_w2v(name, wd):    \n",
    "    os.chdir(wd)\n",
    "    f = pd.read_csv(name, encoding='utf-8')\n",
    "    df = pd.DataFrame(f['614083 300'].str.split(' ',1).tolist(),\n",
    "                                     columns = ['flips','row'])\n",
    "    df = df.rename(columns={'flips':'word', 'row':'vector'})\n",
    "    \n",
    "    from opencc import OpenCC\n",
    "    s = []\n",
    "    cc = OpenCC('t2s')\n",
    "    for i in tqdm(df['word']):\n",
    "        s.append(cc.convert(i))\n",
    "    df['simp'] = s\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "from itertools import chain\n",
    "# transform word into vector via word2Vec\n",
    "def trans(dataframe, vec):\n",
    "    tmp_all = []\n",
    "    for i in tqdm(dataframe['texts']):\n",
    "        tmp = []\n",
    "        for j in i.split():\n",
    "            if j in vec['word'].to_list():\n",
    "#                 print(j)\n",
    "#                 print(vec[vec['word']==j]['vector'])\n",
    "                match = list(chain(*vec[vec['word']==j]['vector'].str.split().tolist()))\n",
    "                match = list(map(float, match))\n",
    "                tmp.append(match)\n",
    "#                 print(tmp)\n",
    "            else:\n",
    "                continue\n",
    "        avg = [sum(x)/len(x) for x in zip(*tmp)]\n",
    "#         print(avg)\n",
    "        tmp_all.append(avg)\n",
    "    dataframe['W2V'] = tmp_all\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0cd6a72c2914f6d88bda05edb154438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4327cb2ec5142b599d16cd6191a5cca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=300.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\doudi\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.674 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 輸入參數 : 測試集，測試集工作目綠，停用詞目綠\n",
    "test = test_preprocess(r'test_cn.json',\n",
    "                          'C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1',\n",
    "                         'C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\DialEval-1\\\\cn_stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89731b7219fd40d9823fec5f867e0c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=614083.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doudi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3f3b64d1724035b7994303308942fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1290.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'zh_wiki_word2vec_300.txt'\n",
    "wd = 'F:\\\\'\n",
    "W2V = import_w2v(name, wd)\n",
    "\n",
    "test = trans(test, W2V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for i in test['W2V']:\n",
    "    t.append(\" \".join([str(j) for j in i])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['W2V_str'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('C:\\\\Users\\\\doudi\\\\OneDrive\\\\Documents\\\\ntcir15\\\\Dataset\\\\New_DialEval-1test_w2v_cn.csv', index=False, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
